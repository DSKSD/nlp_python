[
 {
  "content": "Machine Learning Unconference, Sgt Augmento, TCP Puzzles, and Open Source PowerShell\n\nOpenAI's Machine Learning Unconference -- Oct 7-8 in SF.\n\nSgt Augmento -- Bruce Sterling's new story about robots taking our jobs. (via Cory Doctorow)\n\nTCP Puzzlers -- good stuff! I loved the C Puzzle Book, and all the other \"yeah, no wait...WHAT?\" style mindbenders.\n\nPowerShell Open Sourced -- it's somewhat a shell, but mostly a scripting language for automation.\n\nContinue reading Four short links: 19 August 2016.",
  "title": "Four short links: 19 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/tPfHQkZy3xM/four-short-links-19-august-2016"
 },
 {
  "content": "Alexa shows what\u2019s possible when conversational interfaces just work.Every once in a while, a product comes along that changes everyone's expectations of what's possible in user interfaces. The Mac. The World Wide Web. The iPhone. Alexa belongs in that elite group of game changers. Siri didn't make it over the hump, despite the buzz it created. Neither did Google Now or Cortana, despite their amazing capabilities and their progress in adoption. (Mary Meeker reports that 20% of Google searches on mobile are now done by voice, and Google Now cards are an essential part of every Android user's experience.) But Alexa has done so many things right that everyone else has missed that it is, to my mind, the first winning product of the conversational era.\nLet me talk you through a sample conversation to show you what I mean.\nI'm standing in the kitchen cooking, hands dirty. \"Alexa, play \u2018Hamilton\u2019.\" (Yes, like everyone else who's seen it, or even heard it once, I'm addicted!) \"Playing songs from the original cast recording of \u2018Hamilton\u2019 ...\" \"Alexa, louder.\" \"Alexa, set a timer for 30 minutes.\" [Music volume goes way down but is still audible while Alexa replies.] \"Setting a timer for 30 minutes.\" [Volume comes back up] ... \"Alexa, what song is that?\" [Again, volume goes down while Alexa replies, then returns to previous volume.] \"\u2019Guns and Ships,\u2019 by Leslie Odom, Jr., Daveed Diggs, Christopher Jackson, Original Broadway Cast of \u2018Hamilton\u2019\"... [Phone rings.] \"Alexa, pause.\" [Scramble to wash hands. Wish Alexa was also the interface to my phone!]\u00a0 [End phone conversation.] \"Alexa, resume.\" \"Alexa, how much time is left?\" \"About 9 minutes and 50 seconds left.\"\nWhat's right about this:\n1. Alexa is always listening, so it's hands-free. Once you get used to talking to the thin air and have a device wake up and respond, the idea of pawing at a screen feels as odd as a phone without a touchscreen did starting in 2007, when we got our first taste of multitouch on the iPhone. Touching a microphone icon to go into speech mode doesn't cut it.\n2. Alexa can handle some degree of state with aplomb. I can \"stack\" multiple interactions, and have \"her\" guess with some accuracy which context a subsequent interaction belongs to. She knows that \"pause\" goes with the music,\u00a0 and \"how much time is left?\" goes with the timer.\n3. I didn't need to be taught many of the possible interactions. I just guessed that they might work, tried them, and discovered that they do. For example, I discovered that I could ask Alexa what was playing when I called up an elderly friend to whom I'd given an Echo to see how she liked it. She did, she said, except for the fact that she didn't always know what music she was hearing. (Since she was afraid she wouldn't know how to use it, I'd given her very simple instructions, like \"Say 'Alexa, play Mozart.'\") \"Why don't you try asking?\" I said. \"Say, 'Alexa, what's playing?'\" Sure enough, Alexa came back with the exact name (and performers and conductor) of the classical piece she was listening to.\n4. The design nuance that the volume simply goes down and Alexa talks over it during an overlapping interaction is one of those \"fit and finish\" touches that are part of what makes a new UI paradigm (like the one represented by the original Mac or the iPhone) hum, and be a thing of beauty.\nLet me contrast this with a similar interaction with Google on my phone.\nFirst off, by default, Google isn't listening on most phones. You have to touch the microphone icon to get it to switch to audio input. This is partly a power issue\u2013unlike an Amazon Echo, the phone has battery life considerations\u2013but it is also a privacy issue. I had a conversation with an Alphabet exec in which I made the point that Amazon had totally stolen a march on them with the Echo. He replied, \"Can you imagine the blowback if it were Google that was always listening to you?\" He had a point. But that's how the future happens. Someone breaks the barrier, does the unthinkable, and then it becomes thinkable for everyone. I believe that we're at that point with always-on listening agents.\nAt least on my Nexus 6P, Google has given me the option to enable always on-listening, even from the phone's lock screen. Apple has done the same with Siri on the iPhone 6. But active listening isn't yet on by default, and I suspect eventually it will be. (I first enjoyed this feature on my Moto X, and it made me truly love the phone. But that feature didn't catch on sufficiently to make the phone a breakout success. And now that I've experienced voice interfaces done right on the Echo, I think I know why.)\nSo let's assume I can wake up my phone hands-free simply by talking with it. Let's replay that interaction, this time on my Nexus 6P.\n\"Ok, Google, play \u2018Hamilton\u2019.\" \"\u2019Hamilton\u2019 is a musical about the life of American fFounding father Alexander Hamilton, with music, lyrics and book by Lin-Manuel Miranda.\" [Fail. Responds with the result of a Google search, despite the obvious \"play\" instruction. Doesn't respond with \"this is not in your library.\" So I try again.] \"Ok, Google, play Bob Dylan.\" [Google Play opens, starts playing Bob Dylan from my library.] Good so far. \"Ok Google, pause.\" Nada. From now on, I'm expected to interact with the app by touching the screen. Have to hit the pause button.\nBut let me try other possible actions while the music plays. \"Ok, Google, what song is playing?\" \"\u2019Obviously 5 Believers\u2019.\" That's promising! But once Google has answered my query about the song, Google Play Music is no longer in the foreground. Some other app or mode has answered my question. So I can't even pause or skip the song with a single touch of the screen. I first have to navigate back to Google Play Music. But even when I do that, I don't get to the control to pause or stop the song. Instead, I get a screen asking me to \"Try Unlimited.\"\u00a0\nThat's just bad interaction design, putting the goals of the platform provider above my own. But even if that intervening screen weren't there, you can see that the handoff model, where the conversational agent passes control to an old-school smartphone app, introduces needless complexity into the interface. The conversational agent needs to remain in the foreground, intercepting requests and routing them to the right app (and if necessary, translating them into the native language of the app so that the user doesn't have to switch modes).\nI touch \"NO THANKS.\" Now I can see and press the pause button.\nBut let's go back to the sample interaction. The music is playing. Can I run the timer over it? \"Ok, Google, set a timer for 10 minutes.\" [Music stops entirely \u2013 a much less pleasing interaction than Alexa's gentle muting \u2013 while the Clock opens, giving me the countdown timer from which I can see how much time is left, or, if I like, touches to stop or change the timer.] Music resumes playing, but now the Clock app is in the foreground.\nAnd when I asked \"Ok, Google, how much time is left?\" the question was passed neither to Google Play, nor the Clock. Instead, Google read to me a search result about a calculation that the earth has 7.79 billion years left in the habitable zone.\nLet me be clear: the raw capabilities that Google brings to the table are far in excess of Alexa's. I can ask Google questions that Alexa doesn't have a hope of answering. \"Ok, Google, how long will it take me to get to Palo Alto?\" \"Traffic is heavy so it will take you an hour and 10 minutes.\" And because of its massive amount of stored data as well as real-time sensor data from my phone, and because of its unparalleled expertise in AI, I expect Google to be able to do many things that are impossible for Alexa.\u00a0 But that's precisely why Google should be studying Alexa's voice UI and emulating it.\nThe user interaction flow between Google's voice interface and its mobile apps is a disaster, as I'm lost in a maze of applications, each of which expects to have control, because the voice agent has never been given authority as conductor of the user experience. I'm forced to switch modes unnecessarily between voice and touch. And when the agent doesn't know what to do, it often invokes clearly unrelated actions. (Alexa does this too occasionally, but far less often. It's much better at saying \"I don't know how to answer the question you just asked.\")\nIn addition to creating a consistent voice-only interaction, Alexa's creators have smartly partitioned the possibility space into domains, each with an understandable set of related tasks and questions that are within the capabilities of the agent. Unlike agents that take the model of \"ask me anything\" and often fail ungracefully (Siri), or trying to guess what I might want and surface that information unasked (Google Now), Amazon has done a brilliant job of information architecture. Let's think deeply about music, and design for key interactions. OK, how about weather? How about a kitchen timer? What can we do to make the device more fun? (\"Alexa, tell me a joke.\")\u00a0 There's real evidence of clear and consistent human design anticipation throughout the product.\u00a0 This allows Alexa to appear more intelligent than she actually is.\nAlexa's creators demonstrate an essential insight for the era in which we'll increasingly be designing interfaces with and for intelligent agents. Remember that your agent is essentially stupid, and use humans to put it in known situations where its limited abilities are sufficient, and users can easily learn about its capabilities.\nHuman-Computer Interaction takes big leaps every once in a while. The next generation of speech interfaces is one of those leaps. Humans are increasingly going to be interacting with devices that are able to listen to us and talk back (and increasingly, they are going to be able to see us as well, and to personalize their behavior based on who they recognize). And they are going to get better and better at processing a wide range of ways of expressing intention, rather than limiting us to a single defined action like a touch, click, or swipe.\nRecent advice says that the hype about conversational interfaces is overdone. \"Bots are better without conversation,\" says Ted Livingston of Kik, a text-message based bot platform.\nI don't buy that. My experience with Alexa on the Amazon Echo convinces me otherwise. Of course, Alexa isn't a chatbot. It's a powerful voice-based service embodied in a special purpose device. It demonstrates that conversational interfaces can work, if they are designed right.\nThat leads me to the title of this piece: What would Alexa do?\nAlexa gives us a taste of the future, in the way that Google did around the turn of the millennium. We were still early in both the big data era and the cloud era, and Google was seen as an outlier, a company with a specialized product that, while amazing, seemed outside the mainstream of the industry. Within a few years, it WAS the mainstream, having changed the rules of the game forever.\nMy work on what I called Web 2.0 over a decade ago was an extended meditation on lessons to be learned from Google (and other pioneers of the emerging generation of web applications, platforms and services.) Eventually, these lessons were seen as required learning for every organization, which had to transform itself or die. In that \"Oh sh*t\" moment, Jeff Jarvis wrote a book called What Would Google Do?, whose cover copy advertised it as \"an indispensable manual for survival and success in today\u2019s Internet-driven marketplace.\" That is, if you didn't figure out how to do what Google does, you were screwed! I feel that way right now about Alexa.\nIf you're making any kind of consumer gadget for the home\u2013a TV, a music system, a thermostat, a security system, a Wi-Fi router, a dishwasher, or a washing machine\u2013you should be asking yourself \"What would Alexa do?\" If you're an automotive executive planning to put a big touchscreen in your upcoming model instead of focusing on voice control, you should be asking yourself \"What would Alexa do?\" If you're a software company, you should be imagining a future in which the devices used to interact with your software are increasingly conversational, and asking yourself \"What would Alexa do?\"\u00a0 Heck, if you're a restaurant or coffee shop with an app that lets people order and pay in advance, you should be asking \"What would Alexa do?\"\nFortunately, Amazon is a platform thinker, and so they've provided a neat set of affordances not just for Alexa users but for Alexa developers. App developers can add \"skills\" to Alexa using the Alexa Skills Kit\u2013for example, once you've added the Lyft skill, you can say, \"Alexa, ask Lyft to call me a car.\" And using the Alexa Voice Service, developers can add voice commands to their own applications. (Google also has a speech API, and so does Microsoft.)\nUnfortunately, there's no design API, so you'll have to pay close attention to the way that Amazon has designed the Alexa interface, constantly asking yourself \"What would Alexa do?\" as you design your speech-enabled application. Designers who carry over too much baggage from the touch screen era, and don't learn to think natively about speech interfaces, are likely to build poorly thought-out hybrid experiences like the one that keeps me from using speech as an effective interface to many of the functions of my Android phone.\nI recently had the \"What would Alexa do?\" conversation with a senior technology leader at Facebook. I was pointing out that Facebook uses AI to curate my news feed, with the notion that by watching my behavior, it can guess what stories I most want to see. But I don't always want to see the same thing, I noted, any more than I want a music player that only provides its own curated stream and doesn't give me any choices.\u00a0 It's true that sometimes I want to listen to music that the service chooses for me, but often, I want to express some choice. So too with Facebook. Rather than trying to decide from all the possible posts from my friends which to show me, give me some affordances for expressing my intention.\nAn Alexa-like Facebook interface would let me say \"Facebook, show me personal updates from my friends,\" and the AI would go to work not trying to divine my taste, but in separating personal updates from links to news stories. At another time, I might say \"Facebook, show me links about politics from my friends,\" or \"Facebook, show me funny videos.\" This is AI put in service of my choices, not trying to replace my choices.\nRight now, if I want Facebook to do any of those things, I can only do it by retraining the algorithm over a period of days, religiously avoiding favoriting or clicking on links of the type I don't want to see while choosing only the type I do want. I can't just switch back and forth!\nWhat Alexa has shown us that rather than trying to boil the ocean with AI and conversational interfaces, what we need to do is to apply human design intelligence, break down the conversation into smaller domains where you can deliver satisfying results, and within those domains, spend a lot of time thinking through the \"fit and finish\" so that interfaces are intuitive, interactions are complete, and that what most people try to do \"just works.\"\nConversational interfaces are only one of many ways that businesses are facing tectonic shifts as a result of new technology. If you want to understand how technologies like AI, robotics, on-demand logistics, and more are going to reshape the business and economic landscape, join me at the Next:Economy Summit, October 10-11 in San Francisco.\nThis piece was originally published on LinkedIn.\nContinue reading What would Alexa do?.",
  "title": "What would Alexa do?",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Mv2TIvJq5Q4/what-would-alexa-do"
 },
 {
  "content": "As a data professional, you are invited to share your valuable insights. Help us gain insight into the demographics, work environments, tools, and compensation of practitioners in our growing field. All responses are reported in aggregate to assure your anonymity. The survey will require approximately 5-10 minutes to complete.All responses to this survey are reported in aggregate to assure your anonymity.\nTake the survey\nLoading...Continue reading Take the 2017 Data Science Salary Survey.",
  "title": "Take the 2017 Data Science Salary Survey",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/MSgZi_dtlQk/take-the-2017-data-science-salary-survey"
 },
 {
  "content": "O\u2019Reilly Bots Podcast: Why AI-driven chatbots are a big deal right now.We\u2019re launching a new pop-up podcast about bots. In this first episode of the O\u2019Reilly Bots Podcast, I\u2019m joined by Peter Skomoroch to talk background: why everyone is suddenly interested in bots and what they promise to do, and what sorts of applications are beginning to emerge.Continue reading What are bots? Here\u2019s the background..",
  "title": "What are bots? Here\u2019s the background.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/4WymHbRN5EU/what-are-bots-heres-the-background"
 },
 {
  "content": "The O\u2019Reilly Design Podcast: \u00a0Designing women, avoiding the buzzword curse, and the F word. In this week\u2019s Design Podcast, I sit down with former president of Frog, Doreen Lorenzo. Lorenzo is currently the director for the Center of Integrated Design at the University of Texas at Austin. We talk about the design in education, women in design, and failing fast versus learning fast. Continue reading Doreen Lorenzo on design becoming the core DNA of an organization.",
  "title": "Doreen Lorenzo on design becoming the core DNA of an organization",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ox4u_wKn918/doreen-lorenzo-on-design-becoming-the-core-dna-of-an-organization"
 },
 {
  "content": "The Discovery System is a portable, personal DNA lab that anyone can use.\nThree years ago, we set out to make personal DNA tools that everyone could use. miniPCR started as a maker project, and after a design and prototyping phase, we launched the DNA Discovery System on Kickstarter in 2014. We exceeded our funding goal threefold, which enabled us to scale up production and mount distribution logistics. We have since built a team of biologists, designers, and engineers that operates out of the Harvard Innovation Launch Lab, a community of entrepreneurs and innovators on Harvard University\u2019s dynamic Allston Campus. Manufacturing operations are located just outside of Boston, from where we\u2019ve now shipped DNA Discovery Systems to hundreds of researchers, educators, and DNA curious individuals in every continent.\nThe miniPCR DNA Discovery System is a portable, personal DNA lab that anyone can use. It contains the fundamental tools of DNA analysis: a PCR thermal cycler, a gel electrophoresis apparatus with a built-in blue-light transilluminator, and a variable volume micropipette. It is portable, user friendly, operable through smartphones or computers, and is priced at $990. Its components are also available individually so users can build-to-fit their DNA labs. For example, lab-based researchers may need greater PCR capacity and educators may chose to start with the simpler techniques of DNA gel electrophoresis and visualization. Field-based users may opt to power miniPCR with rechargeable batteries, and couple it to downstream sequencers foregoing gel electrophoresis components. The miniPCR DNA Discovery System (pictured in Figure\u00a01-1) is truly a modular ecosystem that can be configured to the user\u2019s needs.Continue reading miniPCR: Enabling the era of personal DNA.",
  "title": "miniPCR: Enabling the era of personal DNA",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/3l4eJcDl1IY/minipcr-enabling-the-era-of-personal-dna"
 },
 {
  "content": "Explaining Models, Persona Conversation, Experiments with Investors, and Prolog In Your Javascript\n\nWhy Should I Trust You? (gitxiv) -- In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.\n\n\nA Persona-Based Neural Conversation Model -- We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. (via Ben Lorica)\n\nRandomized Field Experiment in Attracting Early Stage Investors -- The average investor responds strongly to information about the founding team, but not to firm traction or existing lead investors. We provide suggestive evidence that team is not merely a signal of quality, and that investing based on team information is a rational strategy. Altogether, our results indicate that information about human assets is causally important for the funding of early-stage firms, and hence, for entrepreneurial success.\n\n\nLogicJS -- Prolog-like declarative logic programming for Javascript.\n\nContinue reading Four short links: 18 August 2016.",
  "title": "Four short links: 18 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/gLGBEXVY648/four-short-links-18-august-2016"
 },
 {
  "content": "Five Questions for Kief Morris: Thoughts on infrastructure as code and the future of infrastructure automation.Kief Morris has been designing, building, and running automated IT server infrastructure for nearly twenty years, having started out with shell scripts and Perl, moving on to CFengine, Puppet, Chef, and Ansible among other technologies as they\u2019ve emerged. He is the head of ThoughtWorks\u2019 European practice for Continuous Delivery and DevOps, helping clients find more effective ways of building and managing infrastructure operations. Kief is the author of Infrastructure as Code, and will be giving a tutorial at Velocity New York in September. I sat down with Kief to discuss his thoughts on infrastructure. Here are some highlights from our conversation.\nWhat is \"Infrastructure as Code\"?\nInfrastructure as Code (IaC)\u00a0is about applying tools and practices from software development to managing infrastructure. It leverages tools\u2014like Ansible, Chef, Puppet and Terraform\u2014that define elements of infrastructure and its configuration in code, which can then be checked into source control, automatically tested using Test Driven Development and Continuous Integration, and safely rolled out to systems using Continuous Delivery.\nWhat is the relationship between IaC and DevOps?\nDevOps is often described with the \u201cCALMS\u201d model, meaning it\u2019s a combination of Culture, Automation, Lean, Measurement, and Sharing. Infrastructure as Code is the \u201cAutomation\u201d part of this.\nWhat are some common anti-patterns for automating infrastructure?\nWhen I started out I did many of the things I see other people do today\u2014for instance, treating server configuration tools like Puppet and Chef as a glorified scripting language. I created loads of identical servers but didn\u2019t use the tools very well to keep things up to date and consistently configured, so everything became a bit of a mess.\nWhat do you see on the horizon for infrastructure automation?\nContainers and PaaS simplify how applications and services interact with infrastructure. I think configuring and managing servers, even virtual servers, will become more of a utility, the way physical hardware is today. So the attention will go into how to build, test, deploy and integrate containerized services. We have a ways to go before things like monitoring, data management systems, authentication and secret management are configured and deployed in a way that\u2019s consistent and testable. So infrastructure as code needs to move up the stack.\nYou're speaking at the Velocity Conference in New York this September. What presentations are you looking forward to attending while there?\nI\u2019m interested in hearing the latest about how different people are handling container orchestration on cloud platforms. Interest in this area has been exploding but it\u2019s still very immature, so there\u2019s a lot to learn. I\u2019m also looking forward to hearing what people are doing to make their environments secure. Automation and cloud have the potential to create highly secure environments, but most people aren\u2019t doing this very well.\nContinue reading What is successful automation?.",
  "title": "What is successful automation?",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/a6MaG5snaFs/what-is-successful-automation"
 },
 {
  "content": "The O\u2019Reilly Security Podcast: The chilling effects of DRM, nascent pro-security industries, and the narrative power of machines.In this episode, I talk with Cory Doctorow, a journalist, activist, and science fiction writer.\nWe discuss the EFF lawsuit against the U.S. government, the prospect for a whole new industry of pro-security businesses, and the new W3C DRM specification.Continue reading Cory Doctorow on legally disabling DRM (for good).",
  "title": "Cory Doctorow on legally disabling DRM (for good)",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/T4YwvdglnBE/cory-doctorow-on-legally-disabling-drm-for-good"
 },
 {
  "content": "Learn best practices for one-on-one meetings that promote productivity and team loyalty within your business\nThese regular meetings improve team communication, identify fixable issues before they transform into big problems, and increase employee loyalty. \nWonderful idea. But when you\u2019re sitting there, staring at each other, what are you supposed to say?Continue reading The Secrets Behind Great One-on-One Meetings.",
  "title": "The Secrets Behind Great One-on-One Meetings",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/_4gJZOCE6ug/the-secrets-behind-great-one-on-one-meetings"
 },
 {
  "content": "Thinking Mathematics, Document Rectification, Human Misjudgement, and Joel Test for Data Science\n\nWhat It's Like to Know Higher Mathematics (Quora) -- a fascinating glimpse at how mathematicians think. [W]hen you do have a deep understanding, you have solved the problem and it is time to do something else. This makes the total time you spend in life reveling in your mastery of something quite brief. One of the main skills of research scientists of any type is knowing how to work comfortably and productively in a state of confusion.\n\n\nFast Document Rectification and Enhancement (Dropbox) -- how this useful thing is done, for image processing enthusiasts.\n\nThe Psychology of Human Misjudgement (PDF) -- interesting run-through of the fallibilities in our judgement. (Think: cognitive biases)\n\nThe Joel Test for Data Science -- eight simple questions intended as a quick-and-dirty sniff test for the quality of a data science team.\n\nContinue reading Four short links: 17 August 2016.",
  "title": "Four short links: 17 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/hH6dKlcAz1Y/four-short-links-17-august-2016"
 },
 {
  "content": "Katie Kent and Jonathan Dinu discuss topics you may be asked about in data science interviews, depending on the types of data science jobs you interview for.Continue reading Common questions in data science interviews.",
  "title": "Common questions in data science interviews",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/njChXL0x72E/common-questions-in-data-science-interviews"
 },
 {
  "content": "Examine the benefits of microservice architecture and techniques to achieve them.\nThe microservice architectural style was defined based on common patterns observed across a number of pioneering organizations. These organizations did not consciously implement a microservice architecture. They evolved to it in pursuit of specific goals.\nIn this chapter, we will explore the common benefits of microservice architecture and how they drive the higher-order goals from not available\u2014speed, safety, and scale; illustrate how the goals of microservice architecture deliver business value; define a maturity model for microservice architecture benefits and goals; and finally, apply this information using a goal-oriented approach to microservice architecture.Continue reading The microservices value proposition.",
  "title": "The microservices value proposition",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/rSpYLDigxxU/the-microservices-value-proposition"
 },
 {
  "content": "Find out what pays and what doesn't for European software engineers, developers, and other programming professionals.\nExecutive Summary\nIN 2016, O\u2019REILLY MEDIA CONDUCTED A SOFTWARE DEVELOPMENT SALARY SURVEY ONLINE. The survey contained 72 questions about the respondents\u2019 roles, tools, compensation, and demographic background. More than 5,000 software engineers, developers, and other professionals involved in programming participated in the survey, 1,353 of them from European countries. This provided us with the opportunity to explore the software-development world\u2014and the careers that propel it\u2014in great detail. Some key findings include:\n\nTop languages currently used professionally in the sample: JavaScript, HTML, CSS, Java, Bash, and Python.\nRespondents reported using an average of 3.6 languages.\nThe highest salaries are in Switzerland, the UK, Ireland, Denmark, and Norway.\nSoftware development is a social endeavor: people who are on tiny teams and who don\u2019t attend meetings tend to earn much less.\nThe most common languages that respondents used in the past but no longer use were C/C++, Java, and PHP.\nThe most common languages that respondents stated they intend to learn in the next 1\u20132 years were Go, Swift, Python, and Scala.\nSalary estimates can be obtained from a model based on the survey data whose coefficients are mentioned throughout the report and repeated in full at the end. We hope you will learn something new (and useful!) from this report, and we encourage you to try plugging your own data points into the model.\n\nIf you are a developer, you may be wondering, \u201cWhat\r\nshould I be earning?\u201d Or at least, \u201cWhat do other people\r\nwith work similar to mine earn?\u201d To satisfy this curiosity, at\r\nthe end of this report, we have provided a way to do a salary\r\nestimate. Our model is based on the survey data whose\r\ncoefficients are mentioned throughout the report. We\r\nhope you will learn something new (and useful) from this\r\nreport, and encourage you to try plugging your own data\r\npoints into the model.Continue reading 2016 European Software Development Salary Survey.",
  "title": "2016 European Software Development Salary Survey",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/dIH5Fi-tm90/2016-european-software-development-salary-survey"
 },
 {
  "content": "Learn how to achieve faster startup and deployments on both Windows, Mac OS X, and Linux, and understand how these containers improve portability across machines.\nPreface\nThe Java programming language was created over 20 years ago. It continues to be the most popular and widely used programming language after all these years. The design patterns and antipatterns of Java deployment are well known. The usual steps to deploy a Java application involve using a script that downloads and installs the operating system package such as JDK on a machine\u2014whether physical or virtual. Operating system threads and memory need to be configured, the network needs to be set up, the correct database identified, and several other such requirements need to be configured for the application to work. These applications are typically deployed on a virtual machine (VM). Starting up these VMs is an expensive operation and can take quite a few minutes in most cases. The number of VMs that can run on a host is also limited because the entire operating system needs to be started, and thus there are stringent requirements on CPU and memory of the host.\nContainers provide several benefits over traditional VM-based deployments. Faster startup and deployments, security and network sandboxing, higher density, and portability across different environments are some of the commonly known advantages. They also improve portability across machines and reduce the impedance mismatch between dev, test, and prod environments.Continue reading Docker for Java developers.",
  "title": "Docker for Java developers",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/_sT88hpEka4/docker-for-java-developers"
 },
 {
  "content": "Rohit Jain takes an in-depth look at the possibilities and the challenges for companies that long for a single query engine to rule them all. \nThe Swinging Database Pendulum\nIt often seems like the IT industry sways back and forth on technology decisions.\nAbout a decade ago, new web-scale companies were gathering more data than ever before and needed new levels of scale and performance from their data systems. There were Relational Database Management Systems (RDBMSs) that could scale on Massively-Parallel Processing (MPP) architectures, such as the following:Continue reading In Search of Database Nirvana.",
  "title": "In Search of Database Nirvana",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ZuWGofxkyyg/in-search-of-database-nirvana"
 },
 {
  "content": "Garrett Grolemund demonstrates how to use R Markdown to combine code and text into a single .Rmd file to generate polished reports automatically in a variety of formats.Continue reading Easy, reproducible reports with R.",
  "title": "Easy, reproducible reports with R",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/b6EE5hanUag/easy-reproducible-reports-with-r"
 },
 {
  "content": "Inferential Thinking, Free Coding School, Technical Wealth, and Social Linked Data\n\nComputational and Inferential Thinking -- textbook for the Foundations of Data Science class at UC Berkeley.\n\n\nArs Covers 42 -- free coding school with a really interesting approach (aims to provide social mobility). The measure will be its failures, not its successes, so I'm keen to see more reporting on it as it expands out. Very ambitious!\n\nBuilding Technical Wealth -- work to fix things that make developers less productive because the value from today's development compounds.\n\nMIT Solid -- (derived from \"social linked data\") is a proposed set of conventions and tools for building decentralized social applications based on Linked Data principles. Solid is modular and extensible, and it relies as much as possible on existing W3C standards and protocols. TimBL's latest project.\n\nContinue reading Four short links: 16 August 2016.",
  "title": "Four short links: 16 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/9T3URVRbZdM/four-short-links-16-august-2016"
 },
 {
  "content": "Navigating the accelerating velocity of change in DevOps.The velocity of change in IT continues to increase. This became a serious challenge for security and compliance with Agile development teams delivering working software in one- or two-week sprints. But the speed at which some DevOps shops initiate and deliver changes boggles the mind. Organizations like Etsy are pushing changes to production 50 or more times each day. Amazon has thousands of small (\u201ctwo pizza\u201d) engineering teams working independently and continuously deploying changes across their infrastructure. In 2014, Amazon deployed 50 million changes: that\u2019s more than one change deployed every second of every day.[1]\nSo much change so fast...\nHow can security possibly keep up with this rate of change? How can they understand the risks, and what can they do to manage them when there is no time to do pen testing or audits, and no place to put in control gates, and you can\u2019t even try to add a security sprint or a hardening sprint in before the system is released to production?\nUse the speed of continuous delivery to your advantage\nThe speed at which DevOps moves can seem scary to infosec analysts and auditors. But security can take advantage of the speed of delivery to respond quickly to security threats and deal with vulnerabilities.\nA major problem that almost all organizations face is that even when they know that they have a serious security vulnerability in a system, they can\u2019t get the fix out fast enough to stop attackers from exploiting the vulnerability.\nThe longer vulnerabilities are exposed, the more likely the system will be, or has already been, attacked. WhiteHat Security, which provides a service for scanning websites for security vulnerabilities, regularly analyzes and reports on vulnerability data that it collects. Using data from 2013 and 2014, WhiteHat found that 35 percent of finance and insurance websites are \u201calways vulnerable,\u201d meaning that these sites had at least one serious vulnerability exposed every single day of the year. The stats for other industries and government organizations were even worse. Only 25 percent of finance and insurance sites were vulnerable for less than 30 days of the year. On average, serious vulnerabilities stayed open for 739 days, and only 27 percent of serious vulnerabilities were fixed at all, because of the costs and risks and overhead involved in getting patches out.\nContinuous Delivery, and collaboration between developers and operations and infosec staff working closely together, can close vulnerability windows. Most security patches are small and don\u2019t take long to code. A repeatable, automated Continuous Delivery pipeline means that you can figure out and fix a security bug or download a patch from a vendor, test to make sure that it won\u2019t introduce a regression, and get it out quickly, with minimal cost and risk. This is in direct contrast to \u201chot fixes\u201d done under pressure that have led to failures in the past.\nSpeed also lets you make meaningful risk and cost trade-off decisions. Recognizing that a vulnerability might be difficult to exploit, you can decide to accept the risk temporarily, knowing that you don\u2019t need to wait for several weeks or months until the next release, and that the team can respond quickly with a fix if it needs to.\nSpeed of delivery now becomes a security advantage instead of a source of risk.\nThe honeymoon effect\nThere appears to be another security advantage to moving fast in DevOps. Recent research shows that smaller, more frequent changes can make systems safer from attackers by means of the \u201cHoneymoon Effect\u201d: older software that is more vulnerable is easier to attack than software that has recently been changed.\nAttacks take time. It takes time to identify vulnerabilities, time to understand them, and time to craft and execute an exploit. This is why many attacks are made against legacy code with known vulnerabilities. In an environment where code and configuration changes are rolled out quickly and changed often, it is more difficult for attackers to follow what is going on, to identify a weakness, and to understand how to exploit it. The system becomes a moving target. By the time attackers are ready to make their move, the code or configuration might have already been changed and the vulnerability might have been moved or closed.\nTo some extent relying on change to confuse attackers is \u201csecurity through obscurity,\u201d which is generally a weak defensive position. But constant change should offer an edge to fast-moving organizations, and a chance to hide defensive actions from attackers who have gained a foothold in your system, as Sam Guckenheimer at Microsoft explains:\n\u201cIf you\u2019re one of the bad guys, what do you want? You want a static network with lots of snowflakes and lots of places to hide that aren\u2019t touched. And if someone detects you, you want to be able to spot the defensive action so that you can take countermeasures.... With DevOps, you have a very fast, automated release pipeline, you\u2019re constantly redeploying. If you are deploying somewhere on your net, it doesn\u2019t look like a special action taken against the attackers.\u201d\n\n\n\n[1] \u201cAWS re:Invent 2015 | (DVO202) DevOps at Amazon: A Look at Our Tools and Proesses.\u201d https://www.youtube.com/watch?v=esEFaY0FDKc\n\n\nContinue reading How continuous delivery helps security keep up with change.",
  "title": "How continuous delivery helps security keep up with change",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/WWwKbYL6GLg/the-security-advantage-of-speed-of-delivery"
 },
 {
  "content": "A systemic approach to building scalable applications.Building a scalable application that has high availability is not easy and does not come automatically. Problems can crop up in unexpected ways, that can cause your beautifully functioning application to stop working for all or some of your customers.\nThese availability problems often arise from the areas you least expect, and some of the most serious availability problems can originate from extremely benign sources. No one can anticipate where problems will come from, and no amount of testing will find all issues. Many of these are systemic problems, not merely code problems.\nTo find these availability problems, we need to step back and take a systemic look at your application and how it works. Here are five things you can and should focus on when building a system to make sure that, as its use scales upwards, availability remains high:\n\nBuild with failure in mind\nAlways think about scaling\nMitigate risk\nMonitor availability\nRespond to availability issues in a predictable and defined way\n\nLet\u2019s look at each of these individually.\nTip #1: Build with failure in mind\nAs Werner Vogels, CTO of Amazon, says, \u201cEverything fails all the time.\u201d Plan on your applications and services failing. It will happen. Now, deal with it.\nAssuming your application will fail, how will it fail? As you build your system, consider availability concerns during all aspects of your system design and construction. For example:\nDesign \nWhat design constructs and patterns have you considered or are you using that will help improve the availability of your software?\nUsing design constructs and patterns, such as simple error catching deep within your application, retry logic, and circuit breakers in a way that allows you to catch errors when they have affected the smallest available subset of functionality. This allows you to limit the scope of a problem and have your application still provide useful capabilities even if part of the application is failing.\nDependencies \nWhat do you do when a component you depend on fails? How do you retry? What do you do if the problem is an unrecoverable (hard) failure, rather than a recoverable (soft) failure?\nCircuit breaker patterns are particularly useful for handling dependency failures because they can reduce the impact a dependency failure has on your system.\nWithout a circuit breaker, you can decrease the performance of your application because of a dependency failure (for example, because an unacceptably long timeout is required to detect the failure). With a circuit breaker, you can \u201cgive up\u201d and stop using a dependency until you are certain that dependency has recovered.\nCustomers \nWhat do you do when a component that is a customer of your system behaves poorly? Can you handle excessive load on your system? Can you throttle excessive traffic? Can you handle garbage data passed in? What about excessive data?\nSometimes, denial-of-service attacks can come from \u201cfriendly\u201d sources. For example, a customer of your application may see a sudden surge in activity that requires a significant increase in the volume of requests to your application. Alternatively, a bug in your customer\u2019s application may cause them to call your application at an unacceptably high rate. What do you do when this happens? Does the sudden increase in traffic bring your application down? Or can you detect this problem and throttle the request rate, limiting or removing the impact to your application?\nTip #2: Always think about scaling\nJust because your application works now does not mean it will work tomorrow. Most web applications have increasing traffic patterns. A website that generates a certain amount of traffic today might generate significantly more traffic sooner than you anticipate. As you build your system, don\u2019t build it for today\u2019s traffic; build it for tomorrow\u2019s traffic.\nSpecifically, this might mean:\n\nArchitect in the ability to increase the size and capacity of your databases.\nThink about what logical limits exist to your data scaling. What happens when your database tops out in its capabilities? Identify these and remove them before your usage approaches those limits.\nBuild your application so that you can add additional application servers easily. This often involves being observant about where and how state is maintained, and how traffic is routed.\nRedirect static traffic to offline providers. This allows your system to only deal with the dynamic traffic that it is designed to deal with. Using external content delivery networks (CDNs) not only can reduce the traffic your network has to handle, but also allows the efficiencies of scale that CDNs provide in order to get that static content to your customers more quickly.\nThink about whether specific pieces of dynamic content can actually be generated statically. Often, content that appears dynamic is actually mostly static and the scalability of your application can be increased by making this content static. This \u201cdynamic that can be static\u201d data is sometimes hidden where you don\u2019t expect it, as the following tip discusses.\n\nTip #3: Mitigate risk\nKeeping a system highly available requires removing risk from the system. When a system fails, often the cause of the failure could have been identified as a risk before the failure actually occurred. Identifying risk is a key method of increasing availability. All systems have risk in them:\n\nThere is risk that a server will crash.\nThere is risk that a database will become corrupted.\nThere is risk that a returned answer will be incorrect.\nThere is risk that a network connection will fail.\nThere is risk that a newly deployed piece of software will fail.\n\nKeeping a system available requires removing risk. But as systems become more and more complicated, this becomes less and less possible. Keeping a large system available is more about managing what your risk is, how much risk is acceptable, and what you can do to mitigate that risk.\nWe call this risk management,[1] and it is at the heart of building highly available systems.\nPart of risk management is risk mitigation. Risk mitigation is knowing what to do when a problem occurs in order to reduce the impact of the problem as much as possible. Mitigation is about making sure your application works as best and as completely as possible, even when services and resources fail. Risk mitigation requires thinking about the things that can go wrong, and putting a plan together, now, to be able to handle the situation when it does happen. Risk management is the process of identifying the risk, determining what to do, and implementing these mitigations.\nThis process will often uncover unknown problems in your application that you will want to fix immediately instead of waiting for them to occur. It also can create processes and procedures to handle known failure modes so that the cost of that failure is reduced in duration or severity.\nAvailability and risk management go hand in hand. Building a highly available system is significantly about managing risk.\nTip #4: Monitor availability\nYou can\u2019t know if there is a problem in your application unless you can see there is a problem. Make sure your application is properly instrumented so that you can see how the application is performing from an external perspective as well as internal monitoring.\nProper monitoring depends on the specifics of your application and needs, but usually entails some of the following capabilities:\nServer monitoring\nTo monitor the health of your servers and make sure they keep operating efficiently.\nConfiguration change monitoring\nTo monitor your system configuration to identify if and when changes to your infrastructure impact your application.\nApplication performance monitoring \nTo look inside your application and services to make sure they are operating as expected.\nSynthetic testing \nTo examine in real time how your application is functioning from the perspective of your users, in order to catch problems customers might see before they actually see them.\nAlerting \nTo inform appropriate personnel when a problem occurs so that it can be quickly and efficiently resolved, minimizing the impact to your customers.\nThere are many good monitoring systems available, both free and paid services. I personally recommend New Relic. It provides all of the aforementioned monitoring and alerting capabilities. As a Software as a Service (SaaS) offering, it can support the monitoring needs at pretty much any scale your application may require.[2]\nAfter you have started monitoring your application and services, start looking for trends in your performance. When you have identified the trends, you can look for outliers and treat them as potential availability issues. You can use these outliers by having your monitoring tools send you an alert when they are identified, before your application fails. Additionally, you can track as your system grows and make sure your scalability plan will continue to work.\nEstablish internal private operational goals for service-to-service communications, and monitor them continuously. This way, when you see a performance or availability-related problem, you can quickly diagnose which service or system is responsible and address the problem Additionally, you can see \u201chot spots\u201d\u2014areas where your performance is not what it could be\u2014and put development plans in place to address these issues.\nTip #5: Respond to availability issues in a predictable and defined way\nMonitoring systems are useless unless you are prepared to act on the issues that arise. This means being alerted when problems occur so that you can take action. Additionally, you should establish processes and procedures that your team can follow to help diagnose issues and easily fix common failure scenarios.\nFor example, if a service becomes unresponsive, you might have a set of remedies to try to make the service responsive. This might include tasks such as running a test to help diagnose where the problem is, restarting a daemon that is known to cause the service to become unresponsive, or rebooting a server if all else fails. Having standard processes in place for handling common failure scenarios will decrease the amount of time your system is unavailable. Additionally, they can provide useful follow-up diagnosis information to your engineering teams to help them deduce the root cause of common ailments.\nWhen an alert is triggered for a service, the owners of that service must be the first ones alerted. They are, after all, the ones responsible for fixing any issues with their service. However, other teams who are closely connected to the troubled service and depend on it might also want to be alerted of problems when they occur. For example, if a team makes use of a particular service, they may want to know when that service fails so that they can potentially be more proactive in keeping their systems active during the dependent service outage.\nThese standard processes and procedures should be part of a support manual available to all team members who handle oncall responsibility. This support manual should also contain contact lists for owners of related services and systems as well as contacts to call to escalate the problem if a simple solution is not possible.\nAll of these processes, procedures, and support manuals should be prepared ahead of time so that during an outage your oncall personnel know exactly what to do in various circumstances to restore operations quickly. These processes and procedures are especially useful because outages often occur during inconvenient times such as the middle of the night or on weekends\u2014times when your oncall team might not perform at peak mental efficiency. These recommendations will assist your team in making smarter and safer moves toward restoring your system to operational status.\nBe prepared\nNo one can anticipate where and when availability issues will occur. But you can assume that they will occur, especially as your system scales to larger customer demands and more complex applications. Being prepared in advance to handle availability concerns is the best way to reduce the likelihood and severity of problems. The five techniques discussed here offer a solid strategy for keeping your applications highly available.\n\n\n\n[1] Risk management is discussed extensively in Part II of Architecting for Scale.\n\n\n[2] I should point out that I work at New Relic, but this is not why I recommend it. I discovered and started using the New Relic tools before I started working there. My success in using its tools to solve my performance and availability problems is why I started working for New Relic, not the other way around.\n\n\nContinue reading 5 tips for improving availability.",
  "title": "5 tips for improving availability",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/kpGXzxgask0/5-tips-for-improving-availability"
 },
 {
  "content": "The O\u2019Reilly Podcast: Sean P. Kane discusses how to get your team using Docker in the real world.Spinning up containers is one thing, but how do you actually use Docker as a team? This episode of the O\u2019Reilly Podcast features my discussion with Sean P. Kane, lead site reliability engineer at New Relic. We talk about what Docker offers to your team, and how you can realistically adopt it in your organization.\nThe full conversation is available through the embedded audio. Highlights from the discussion are noted below.\n\nCommon misconceptions people bring to Docker\n\nObviously, like any tool, Docker is not going to solve all of your problems. It's not some magical tool that makes life perfect, but used correctly it can put a serious dent in a lot of the problems people have around deployment workflow\u2014basically that whole pipeline. Docker isn't virtualization; that's a common misconception. It doesn't actually virtualize at all. All your processes are running on top of the Linux kernel directly. So, as an example, in something like the VMware or KVM you could run Windows on top of a Linux server, but with Docker you can actually only run Linux binaries inside a Linux container because it's still running natively on top of the kernel.\n\nThe real value proposition of Docker\n\nIn a phrase: it\u2019s the development pipeline. Streamlining your development pipeline is a big plus.\nThere's also this ability to take all these application dependencies, combine them all together, and deliver them basically in one large artifact that doesn't necessarily need to be a supply to any other team. It could just be once a developer has potentially pushed it up to the repository, it can be pulled down by your Jenkins job automatically and used, and then even pulled down if you had a deployment process as well, and then deployed out into production. Docker can make it incredibly simple to redeploy applications during regular operations or emergencies.\nIn our environment we wrote a small wrapper tool for some of our work called Centurion, which is an open-sourced project intended to be a very simple client for developers to use to manage and deploy their applications to Docker. This was long before most of the orchestration tools existed, or were widely used for Docker. That tool allowed us as a company to empower anyone to redeploy an application, even without knowing anything about the application.\nIf I got paged in the middle of the night as an operations engineer and it was obvious that service X was in a bad state and maybe we couldn't get ahold of the on calls for those teams or it was just urgent and we couldn't wait fifteen minutes for somebody to get online, I knew that I could at least try to redeploy the current version or even potentially do a rollback to the previous version if a deploy had been done recently and we thought that this was code related. I could do that as an operations engineer, very easily and reliably, and I knew that it was going to work.\nWhereas if I\u2019d attempted it before, there were a hundred services out there and they all were done with their special developer magic, and some people used Capistrano and other people used some other deployment tooling. It was very difficult for those of us on the operations team to actually make a decision like: We're just going to redeploy this to see if we can get it back into a healthy state. With Docker and Centurion in our case, that became much easier to do.\n\nThree tips for moving your organization to Docker\n\n(1) Don't rush, but don't really hold back either. If you look at your processes, try to find the worst pain point in your current deployment process pipeline. Maybe it's your testing. Focus on that. Try to see how you can potentially use Docker to help make that small piece better.\n(2) Start with a simple problem. In our case it was, \u201cHow can we make it easier for developers to deploy in a more repeatable fashion?\u201d We just focused on that initially, and that was the first thing we rolled out. Then we built upon that. You can deploy something very simple to start with. Maybe your developers don't even use Docker, but you're going to use Jenkins to take the code base and build it inside a Docker container and do all the testing inside a container. Verify that the test works, and then destroy the container. Start there, with that very focused thing, and then expand from there.\n(3) Start static and then evolve to a truly dynamic environment, leveraging orchestration technologies like Kubernetes. Unless you have really experienced operation engineers in your organization, jumping feet first into a dynamic environment may be biting off more than you can chew. It doesn't allow you to spend time actually getting to understand the technology of Docker and what it's good at. So, focus a bit on Docker first, and migrate up to large scale orchestration.\n\nWhat\u2019s the best way to get started working with containers?\n\nDocker is pretty easy to get started with. It's harder to completely understand how you can best use it. First take a look at is Docker Engine, the Docker client. This is the tool used to do things like build a Docker image, push that image up to a repository, pull it back down and run it. It's the easiest tool to get started with.\nThere's a lot of good documentation and tutorials out there about how to use it. And then of course both the book that Karl Matthias and I wrote for O\u2019Reilly, Docker: Up and Running, and my online class are great places for people who want to really quickly get their feet wet and understand both the tools and the overall way to approach and think about Docker.\n\nContinue reading Thinking with containers: 3 tips for moving to Docker.",
  "title": "Thinking with containers: 3 tips for moving to Docker",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/RATU5aYWEsw/thinking-with-containers--3-tips-for-moving-to-docker"
 },
 {
  "content": "Building cloud-native apps that start rapidly and shut down gracefully.In Beyond the Twelve-Factor App, I present a new set of guidelines that builds on Heroku\u2019s original 12 factors and reflects today\u2019s best practices for building cloud-native applications. I have changed the order of some to indicate a deliberate sense of priority, and added factors such as telemetry, security, and the concept of \u201cAPI first\u201d that should be considerations for any application that will be running in the cloud. These new 15-factor guidelines are:\n\nOne codebase, one application\nAPI first\nDependency management\nDesign, build, release, and run\nConfiguration, credentials, and code\nLogs\nDisposability\nBacking services\nEnvironment parity\nAdministrative processes\nPort binding\nStateless processes\nConcurrency\nTelemetry\nAuthentication and authorization\n\nDisposability is the ninth of the original 12 factors.\nOn a cloud instance, an application\u2019s life is as ephemeral as the infrastructure that supports it. A cloud-native application\u2019s processes are disposable, which means they can be started or stopped rapidly. An application cannot scale, deploy, release, or recover rapidly if it cannot start rapidly and shut down gracefully. We need to build applications that not only are aware of this, but also embrace it to take full advantage of the platform.\nThose used to developing in the enterprise world with creatures like application containers or large web servers may be used to extremely long startup times measured in minutes. Long startup times aren\u2019t limited to just legacy or enterprise applications. Software written in interpreted languages or just written poorly can take too long to start.\nIf you are bringing up an application, and it takes minutes to get into a steady state, in today\u2019s world of high traffic that could mean hundreds or thousands of requests get denied while the application is starting. More importantly, depending on the platform on which your application is deployed, such a slow startup time might actually trigger alerts or warnings as the application fails its health check. Extremely slow startup times can even prevent your app from starting at all in the cloud.\nIf your application is under increasing load, and you need to rapidly bring up more instances to handle that load, any delay during startup can hinder its ability to handle that load. If the app does not shut down quickly and gracefully, that can also impede the ability to bring it back up again after failure. Inability to shut down quickly enough can also run the risk of failing to dispose of resources, which could corrupt data.\nMany applications are written such that they perform a number of long-running activities during startup, such as fetching data to populate a cache or preparing other runtime dependencies. To truly embrace cloud-native architecture, this kind of activity needs to be dealt with separately. For example, you could externalize the cache into a backing service so that your application can go up and down rapidly without performing front-loaded operations.\nContinue reading Embracing disposable processes.",
  "title": "Embracing disposable processes",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/wPWrsCyLUv0/embracing-disposable-processes"
 },
 {
  "content": "Learn about Clair, a new open source tool to monitor the security of containers and automatically detect vulnerabilities in Docker and rkt containers.Continue reading Clair: Clarity with container security scanning.",
  "title": "Clair: Clarity with container security scanning",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/w2RkpQ4RCxg/clair-clarity-with-container-security-scanning"
 },
 {
  "content": "The British have a way of turning their backs on their greatest innovations.Despite having invented many of the foundations of modern computing, Alan Turing was convicted of indecency and eventually took his own life. The torch of the computer revolution passed to the Institute for Advanced Studies and other pioneers in the U.S. When Tim Berners-Lee invented the World Wide Web, he was given a knighthood, but nonetheless ended up at MIT rather than at Cambridge or Oxford.\nNow, Britain is at it again. The U.K. Government Digital Service, hailed and emulated around the world as among the most innovative and successful approaches to bringing government into the 21st century, is the target of senior civil servants who seek to break it up and reduce its ability to transform business as usual. Since the initial news, the story has been confirmed by the departure of GDS head Stephen Foreshew-Cain and commentary on the \u201ccoup\u201d from a former insider.\nLet\u2019s be clear. There are plenty of reasons to break up and disempower the Government Digital Service\u2014if you\u2019re a vendor who benefits from a system where IT projects routinely cost hundreds of times what they ought, take years longer, and often fail completely. But if you\u2019re a civil servant charged with maximizing the public good, this is a very, very poor idea.\nGOV.UK, the site built by the Government Digital Service, replaced thousands of fragmented websites with a single service that has had more usage by citizens than all of the sites it replaced, won plaudits for its ease of use, and saved HMS government over 60 million pounds last year alone. The GDS approach of iterative, data-driven, user-centric design results in services that actually work, are delivered on time, and increase citizens\u2019 satisfaction with their government.\nOther countries know a good thing when they see it!\nThe work of the GDS was the direct inspiration for the United States Digital Service (USDS), the unit that was established to continue the work of the team that rescued healthcare.gov, and with it, the signature policy achievement of the Obama administration.\nThe plans for the USDS were already well underway when the healthcare.gov crisis drove home the value of a central team of technologists within government, with the power not just to direct technology policy but actually to implement key systems. My wife, Jennifer Pahlka, had accepted U.S. Chief Technology Officer Todd Park's offer to become his deputy, on condition that she be allowed to draw up plans to create a service modeled on the U.K.\u2019s Government Digital Service. And Ezra Klein\u2019s Bloomberg Business Week cover story contrasting the healthcare.gov failure with the work of the GDS drove the point home. President Obama himself acknowledged the inspiration of the U.K. Government Digital Service in a letter to GDS founder Mike Bracken after Bracken stepped down.\n\nThe USDS and 18F, a sister group set up at the same time at the General Services Administration, have been having a huge impact here in the U.S. by following the GDS playbook. The USDS Digital Services Playbook closely follows the U.K. GDS Design Principles and Digital Service Standard. And these efforts are not just saving taxpayers money, they\u2019re saving lives and rebuilding trust and faith with the American public.\nAnd it\u2019s not just in America that the GDS effect is taking hold. The Australian Digital Transformation Office is run by an ex-GDS leader, and its service standard and principles pay homage to GDS. New Zealand, Israel, Mexico, and several other countries are following suit. And yet, despite having a global success story on its hands, the forces of conservatism in the British system appear to want to break up GDS.\nAccording to the article in Computer Weekly that lays out the plans to roll back the authority of the GDS, the intent is \u201cto break up GDS and return to the sort of model that existed before the 2010 general election, where a much smaller central policy team was responsible for strategy and standards, with all delivery returned to Whitehall departments.\u201d This is a serious mistake. It is indeed essential to get the departments involved, but they must not return to the pre-2010 practice of outsourcing IT to vendors rather than building the internal delivery capability that keeps vendors honest and makes sure that the government itself has the ability to deliver on key services.\nRather, the central GDS should be maintained with its current funding, and its digital services playbook copied by the departments. The role of the GDS should not be replaced by work in the departments, but augmented by departmental Digital Services that work closely with the central coordinating team. The central team is essential, because government needs not just common standards, but common, re-usable implementation of key platform technologies. Otherwise, each department tends to recreate the wheel.\nThis is a key lesson that the US has applied, building not just central services in USDS and 18F, but also departmental digital services at each of the major agencies. Even the Veterans Administration, so broken for so long that many had given up on it, has started to turn around by implementing these digital best practices, which differ fundamentally from conventional government IT practices. For example, the VA recently announced they\u2019d revamped the online application form for healthcare for veterans because the previous one literally did not open for most users. The VA\u2019s post about how they made the new application a success was unequivocal: \u201cprioritize users, and engage them early and often.\u201d If that sounds a lot like the first several points of the GDS\u2019s Digital Service Standard, that\u2019s not a coincidence. The Departments of Health and Human Services, Education, Homeland Security, and many others all have similar stories, and they\u2019re starting to add up to something big enough to be the new normal. This is thanks, in no small part, to the example and frequent guidance and support from GDS leadership and staff over the past few years.\nFortunately, the British have only started the process to dismantle and disempower the GDS, by removing its respected delivery leader, Stephen Foreshew-Cain, and by disposing with digital leaders in its biggest departments, folding them back under CIOs and the Big IT crowd. There is still time for the new U.K. government to reconsider, but it will need strong political leadership. They should double down on the enormous gains that the GDS has already made in modernizing government IT instead of throwing away their advantage and looking on in envy as other nations finish what they started.\nContinue reading The UK needs to double down on the GDS, not dismantle it.",
  "title": "The UK needs to double down on the GDS, not dismantle it",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/QYsIPZ-GmHk/the-uk-needs-to-double-down-on-the-gds-not-dismantle-it"
 },
 {
  "content": "Code that Codes, Learn Don't Tell, Traffic Security, and Simulating Taxation\n\nExCAPE -- In the proposed paradigm, a programmer can express insights through a variety of forms such as incomplete programs, example behaviors, and high-level requirements, and the synthesis tool generates the implementation relying on powerful analysis algorithms and programmer collaboration. \"Programs that write programs are the happiest programs in the world.\" -- Andrew Hume. Software making software is the logical end-game. (via ADT)\n\nCartPole -- OpenAI's gym version of a classic better-if-learned-than-told programming problem. For more, see Mat Kelcey.\n\nTraffic Camera Security -- To summarize, we discovered three major weaknesses in the road agency\u2019s traffic infrastructure deployment: 1. The network is accessible to attackers due to the lack of encryption. 2. Devices on the network lack secure authentication due to the use of default usernames and passwords. 3. The traffic controller is vulnerable to known exploits. ... which they went on to exploit.\n\nOpenFisca --  an open micro-simulator of the tax-benefit system. It allows users to calculate many social benefits and taxes paid by households and to simulate the impact of reforms on their budget. Models and interactive-simulations are the best ways to learn about dynamic systems.\n\nContinue reading Four short links: 15 August 2016.",
  "title": "Four short links: 15 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ltxVFDZFa8E/four-short-links-15-august-2016"
 },
 {
  "content": "Lost Infrastructure, Trump as Machine Learning Algorithm, Malicious Proof-of-Work, and Twitter Abuse\n\nThe Lost Infrastructure of Social Media (Anil Dash) -- what we've lost, and what we could get back.\n\nTrump is Like a Biased Machine Learning Algorithm -- an entertaining take on Trump and a cautionary not for people [who] actually think there will soon be algorithms that control us, operating \u201cthrough sound decisions of pure rationality\u201d and that we will no longer have use for politicians at all.\n\n\nDDoSCoin: Cryptocurrency with a Malicious Proof-of-Work -- brilliant Usenix paper! This proof involves making a large number of TLS connections to a target server, and using cryptographic responses to prove that a large number of connections has been made. Like proof-of-work puzzles, these proofs are inexpensive to verify, and can be made arbitrarily difficult to solve.\n\n\nA Honeypot for Assholes -- a history of Twitter's failure to prosecute trolls. I didn't realize how much was inspired by the Free Speech cause in the beginning.  It's interesting just how far the conversation about conversations has progressed since then.\n\nContinue reading Four short links: 12 August 2016.",
  "title": "Four short links: 12 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/lpnRqxEi-ck/four-short-links-12-august-2016"
 },
 {
  "content": "A technique to explain the predictions of any machine learning classifier.Machine learning is at the core of many recent advances in science and technology. With computers beating professionals in games like Go, many people have started asking if machines would also make for better drivers or even better doctors.\nIn many applications of machine learning, users are asked to trust a model to help them make decisions. A doctor will certainly not operate on a patient simply because \u201cthe model said so.\u201d Even in lower-stakes situations, such as when choosing a movie to watch from Netflix, a certain measure of trust is required before we surrender hours of our time based on a model. Despite the fact that many machine learning models are black boxes, understanding the rationale behind the model's predictions would certainly help users decide when to trust or not to trust their predictions. An example is shown in Figure 1, in which a model predicts that a certain patient has the flu. The prediction is then explained by an \"explainer\" that highlights the symptoms that are most important to the model. With this information about the rationale behind the model, the doctor is now empowered to trust the model\u2014or not.\n\nFigure 1. Explaining individual predictions to a human decision-maker. Source: Marco Tulio Ribeiro.\n\nIn a sense, every time an engineer uploads a machine learning model to production, the engineer is implicitly trusting that the model will make sensible predictions. Such assessment is usually done by looking at held-out accuracy or some other aggregate measure. However, as anyone who has ever used machine learning in a real application can attest, such metrics can be very misleading. Sometimes data that shouldn't be available accidentally leaks into the training and into the held-out data (e.g., looking into the future). Sometimes the model makes mistakes that are too embarrassing to be acceptable. These and many other tricky problems indicate that understanding the model's predictions can be an additional useful tool when deciding if a model is trustworthy or not, because humans often have good intuition and business intelligence that is hard to capture in evaluation metrics. Assuming a \u201cpick step\u201d in which certain representative predictions are selected to be explained to the human would make the process similar to the one illustrated in Figure 2.\n\nFigure 2. Explaining a model to a human decision-maker. Source: Marco Tulio Ribeiro.\n\nIn \"Why Should I Trust You?\" Explaining the Predictions of Any Classifier, a joint work by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin (to appear in ACM's Conference on Knowledge Discovery and Data Mining -- KDD2016), we explore precisely the question of trust and explanations. We propose Local Interpretable Model-Agnostic Explanations (LIME), a technique to explain the predictions of any machine learning classifier, and evaluate its usefulness in various tasks related to trust.\nIntuition behind LIME\nBecause we want to be model-agnostic, what we can do to learn the behavior of the underlying model is to perturb the input and see how the predictions change. This turns out to be a benefit in terms of interpretability, because we can perturb the input by changing components that make sense to humans (e.g., words or parts of an image), even if the model is using much more complicated components as features (e.g., word embeddings).\nWe generate an explanation by approximating the underlying model by an interpretable one (such as a linear model with only a few non-zero coefficients), learned on perturbations of the original instance (e.g., removing words or hiding parts of the image). The key intuition behind LIME is that it is much easier to approximate a black-box model by a simple model locally (in the neighborhood of the prediction we want to explain), as opposed to trying to approximate a model globally. This is done by weighting the perturbed images by their similarity to the instance we want to explain. Going back to our example of a flu prediction, the three highlighted symptoms may be a faithful approximation of the black-box model for patients who look like the one being inspected, but they probably do not represent how the model behaves for all patients.\nSee Figure 3 for an example of how LIME works for image classification. Imagine we want to explain a classifier that predicts how likely it is for the image to contain a tree frog. We take the image on the left and divide it into interpretable components (contiguous superpixels).\n\nFigure 3. Transforming an image into interpretable components. Sources: Marco Tulio Ribeiro, Pixabay.\n\nAs illustrated in Figure 4, we then generate a data set of perturbed instances by turning some of the interpretable components \u201coff\u201d (in this case, making them gray). For each perturbed instance, we get the probability that a tree frog is in the image according to the model. We then learn a simple (linear) model on this data set, which is locally weighted\u2014that is, we care more about making mistakes in perturbed instances that are more similar to the original image. In the end, we present the superpixels with highest positive weights as an explanation, graying out everything else.\n\nFigure 4. Explaining a prediction with LIME. Sources: Marco Tulio Ribeiro, Pixabay.\n\nExamples\nWe used LIME to explain a myriad of classifiers (such as random forests, support vector machines (SVM), and neural networks) in the text and image domains. Here are a few examples of the generated explanations.\nFirst, an example from text classification. The famous 20 newsgroups data set is a benchmark in the field, and has been used to compare different models in several papers. We take two classes that are hard to distinguish because they share many words: Christianity and atheism. Training a random forest with 500 trees, we get a test set accuracy of 92.4%, which is surprisingly high. If accuracy was our only measure of trust, we would definitely trust this classifier. However, let\u2019s look at an explanation in Figure 5 for an arbitrary instance in the test set (a one liner in Python with our open source package):\n\r\nexp = explainer.explain_instance(test_example, classifier.predict_proba, num_features=6)\r\n\n\nFigure 5. Explanation for a prediction in the 20 newsgroups data set. Source: Marco Tulio Ribeiro.\n\nThis is a case in which the classifier predicts the instance correctly, but for the wrong reasons. Additional exploration shows us that the word \"posting\" (part of the email header) appears in 21.6% of the examples in the training set but only two times in the class \u201cChristianity.\u201d This is also the case in the test set, where the word appears in almost 20% of the examples but only twice in \u201cChristianity.\u201d This kind of artifact in the data set makes the problem much easier than it is in the real world, where we wouldn't expect such patterns to occur. These insights become easy once you understand what the models are actually doing, which in turn leads to models that generalize much better.\nAs a second example, we explain Google's Inception neural network on arbitrary images. In this case, illustrated in Figure 6, the classifier predicts \u201ctree frog\u201d as the most likely class, followed by \"pool table\" and \"balloon\" with lower probabilities. The explanation reveals that the classifier primarily focuses on the frog's face as an explanation for the predicted class. It also sheds light on why \"pool table\" has non-zero probability: the frog's hands and eyes bear a resemblance to billiard balls, especially on a green background. Similarly, the heart bears a resemblance to a red balloon.\n\nFigure 6. Explanation for a prediction from Inception. The top three predicted classes are \"tree frog,\" \"pool table,\" and \"balloon.\" Sources: Marco Tulio Ribeiro, Pixabay (frog, billiards, hot air balloon).\n\nIn the experiments in our research paper, we demonstrate that both machine learning experts and lay users greatly benefit from explanations similar to Figures 5 and 6 and are able to choose which models generalize better, improve models by changing them, and get crucial insights into the models' behavior.\nConclusion\nTrust is crucial for effective human interaction with machine learning systems, and we think explaining individual predictions is an effective way of assessing trust. LIME is an efficient tool to facilitate such trust for machine learning practitioners and a good choice to add to their tool belts (did we mention we have an open source project?), but there is still plenty of work to be done to better explain machine learning models. We're excited to see where this research direction will lead us. The video below provides an overview of LIME, with more details available in our paper.\n\n\u00a0\nContinue reading Introduction to Local Interpretable Model-Agnostic Explanations (LIME).",
  "title": "Introduction to Local Interpretable Model-Agnostic Explanations (LIME)",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/tShMMgSef0A/introduction-to-local-interpretable-model-agnostic-explanations-lime"
 },
 {
  "content": "Jeff Immelt on GE\u2019s investments in big data, brilliant factories, and the industrial Internet, and the role of humans in the industrial economy of the 21st century.Continue reading GE's digital transformation.",
  "title": "GE's digital transformation",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/6RH-O3v_yjs/ges-digital-transformation"
 },
 {
  "content": "SCRN empowers researchers to easily access tools to analyze or share their data and analysis workflows.\nA Deeper Look: Russell Poldrack and the Center for Reproducible Neuroscience\nWith the meteoric advancement of technology, there is increasing scrutiny about how science is getting done. The Internet has enabled scientific results to be publicized, disseminated, modified, and expanded within minutes. Social media can subsequently propagate that information universally, allowing virtually anyone with access to WiFi to influence\u2014and, therefore, potentially skew\u2014data collection procedures and results. Ultimately, this means that reproducibility of modern science is under attack.Continue reading Where science-as-a-service and supercomputing meet.",
  "title": "Where science-as-a-service and supercomputing meet",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/nxmP9jKDjzY/where-science-as-a-service-and-supercomputing-meet"
 },
 {
  "content": "The O'Reilly Radar Podcast: Natural language understanding and natural language processing applications, our future with chatbots, and open source indexing.This week, I talk with Alyona Medelyan, co-founder and CEO at Thematic and founder and CEO at Entopix. We talk about natural language understanding, the challenges of analyzing unstructured text, and her open source indexing tool Maui that she's been working on for the past 10 years.Continue reading Alyona Medelyan on applications of NLU.",
  "title": "Alyona Medelyan on applications of NLU",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/aghrD7kV6zE/alyona-medelyan-on-applications-of-nlu"
 },
 {
  "content": "The O\u2019Reilly Data Show Podcast: Jana Eggers on building applications that rely on synaptic intelligence.In this episode of the O\u2019Reilly Data Show, I spoke with Jana Eggers, CEO of Nara Logics. Eggers\u2019 involvement with AI dates back to her days as a researcher at the Los Alamos National Laboratory. Most recently she has been helping companies across many industries adopt AI technologies as a way to enable a range of intelligent data applications.Continue reading Enabling enterprise adoption of AI technologies.",
  "title": "Enabling enterprise adoption of AI technologies",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/MDWyZHPmT5I/enabling-enterprise-adoption-of-ai-technologies"
 },
 {
  "content": "Benefits of Driverless Cars, Editing a Genome, Lagging Porn, and Federal Source Code\n\nSelf-Driving Cars Will Improve Our Cities If They Don't Ruin Them -- a reminder that the possible positives of self-driving cars (such as fewer cars) should be regulated for, not assumed as a logical consequence of the introduction of the technology. My 2004 Prius costs me about $1.50 for an hour of run time. It will be cheaper to have my car double-park or circle blocks rather than pay for a parking meter or, heaven forbid, pay for parking in a downtown garage.\n\n\nBeyond CRISPR: Other Ways To Edit a Genome (Nature) -- NgAgo is just one of a growing library of gene-editing tools. Some are variations on the CRISPR theme; others offer new ways to edit genomes.\n\n\nPorn Does Not Lead Technology Any More (Wired) -- Some of it may have been true in years past. But no longer. A colleague of mine calls this a meso-idea, an idea that has ceased to be true but that people continue to repeat, ad infinitum, as if it still was. Sex has been commodified, price racing to the bottom in an ocean of perfect competition as the tools of production are democratized and the Internet destroys the pornography industry the way it took hatchets to the newspaper industry.\n\nThe People's Code -- We\u2019re releasing the Federal Source Code policy to support improved access to custom-developed Federal source code. The policy, which incorporates feedback received during the public comment period, requires new custom-developed source code developed specifically by or for the Federal Government to be made available for sharing and re-use across all Federal agencies. It also includes a pilot program that will require Federal agencies to release at least a portion of new custom-developed Federal source code to the public and support agencies in going beyond that minimum requirement.\n\n\nContinue reading Four short links: 11 August 2016.",
  "title": "Four short links: 11 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Bmg91OhnStg/four-short-links-11-august-2016"
 },
 {
  "content": "Mark Grover and Ted Malaska offer an overview of projects for streaming applications, including Kafka, Flume, and Spark Streaming, and discuss the architectural schemas available, such as Lambda and Kappa.Continue reading Best practices for streaming applications.",
  "title": "Best practices for streaming applications",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/45cP-bNvTcs/best-practices-for-streaming-applications"
 },
 {
  "content": "James Turnbull shares insights on operations, culture, and monitoring.I recently sat down with James Turnbull, CTO at Kickstarter, to discuss how the operations role is changing, and what the future may hold for the industry in general and for monitoring and infrastructure tools in particular. Here are some highlights from our talk.\n1. What makes a good operations professional in this day and age?\nA good operations professional is diverse and flexible. Once upon a time it may have been possible to pigeonhole an \u201coperation\u2019s person,\u201d usually as a cliched stereotype of a bearded white dude, writing 10,000-line Bash scripts in between craft beers and bitterness. Nowadays, when I think about ops folks, I think of business-savvy, deeply technical, thoughtful, polyglot programmers who pride themselves on delivering the right, high quality, automated, measured and managed services. They pride themselves on being part of the engineering community in their organizations, rather than a silo. They think about business requirements, engage with their peers in helping them build deployable and manageable product,\u00a0and conduct blameless post-mortems when things go wrong.\nA good operations professional today cares about user experience, developer happiness, work-life balance, and collaboration. They are learning people who want to make things better in their communities and organizations. They are empathetic to the needs of their colleagues because they own the experience (and the cost!) of putting those colleagues\u2019 hard-built features and product out into the world. They are compassionate and considerate because they have the experience of being on call and of bearing the brunt of customer disappointment if a product or service is unavailable or unacceptable.\nA good operations professional looks like Charity Majors, Aaron Suggs, Katherine Daniels, Justin Lintz, Mandi Walls, Nigel Kersten, Alice Goldfuss, and a myriad of other amazing folks who have made me proud to have an operations heritage.\n2. You've written extensively on the topic of monitoring. What are the most exciting trends you are seeing lately?\nThe most exciting trend I\u2019ve seen is the new focus on ensuring the business needs of companies are addressed by monitoring. A lot of folks have started evangelising that instead of monitoring being built from the bottom up, starting with hosts and infrastructure, they are instead built from the top down, starting with business metrics and applications. You identify the metrics that represent the health of your business, for example sales volume or checkout conversion. You then work down the stack, identifying the applications, services, and finally hosts that contribute to that health. Now if something changes in the health of your business then you\u2019ll not only know about it, but you can also follow any anomalies in the stack to any misbehaving infrastructure.\n3. You've written several books on the topic of infrastructure, and worked at places as varied as Puppet and Docker Inc. What do you see as the future of infrastructure tools?\nI think the future holds some mix of movement of workloads to more services-oriented architecture (and microservices) and movement to more flexible compute platforms like cloud and PaaS. By extension, I think we\u2019ll see more tooling that makes that movement and use of that architecture easier.\nBut I do think it is a bit nebulous right now. I think a really interesting phenomenon at the moment is the long tail of adoption for infrastructure tools. For every company in the Bay or New York City going all in on Docker and Kubernetes there are a hundred just discovering that there might be solutions to the problems of modern infrastructure management.\n4. How does one cultivate and maintain a good engineering culture?\nI could write a book on this and still not cover everything! We start with a fair and blameless culture where people can make mistakes and learn. Good, transparent communication between the team, between peers in the design and product communities, and into the business is critical. So is solid training, learning, and mentoring patterns and approaches across all levels of the team; clear understanding of our objectives, and an individual understanding of what you need to do to be successful; and collective ownership of the future of your product, and of important central concerns like availability, security, and performance. Everyone is accountable for making performant, secure, and resilient product.\n5. You're one of the chairs for the Velocity Conference in New York this September. What presentations are you looking forward to attending while there?\nI think it\u2019s a great program overall. It\u2019s hard to narrow it down to just a few presentations! Some highlights that I am looking forward to are the \u201cDesigning large-scale distributed systems\u201d workshop, the \u201cOps in the time of serverless containerized webscale\u201d panel, the thought-provoking and funny Joe Damato\u2019s \u201cInfrastructure as code might be literally impossible,\u201d and Shopify\u2019s talk on scaling their multi-tenant architecture.\nContinue reading Five questions for James Turnbull.",
  "title": "Five questions for James Turnbull",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ruBw6SEedvs/five-questions-for-james-turnbull"
 },
 {
  "content": "You\u2019ve got three options: Scaling up, scaling out, or using R as an abstraction layer.R is among the top five data science tools in use today according O\u2019Reilly research; the latest kdnuggets survey puts it in first, and IEEE Spectrum ranks it as the fifth most popular programming language.\nThe latest Rexer Data Miner survey revealed that in the past eight years, there has been an three-fold increase in the number of respondents using R, and a seven-fold increase in the number of analysts/scientists who have said that R is their primary tool.\nDespite its popularity, the main drawback of vanilla R is its inherently \u201csingle threaded\u201d nature and its need to fit all the data being processed in RAM. But nowadays, data sets are typically in the range of GBs and they are growing quickly to TBs. In short, current growth in data volume and variety is demanding more efficient tools by data scientists.\nEvery data science analysis starts with preparing, cleaning, and transforming the raw input data into some tabular data that can be further used in machine learning models.\nIn the particular case of R, data size problems usually arise when the input data do not fit in the RAM of the machine and when data analysis takes a long time because parallelism does not happen automatically. Without making the data smaller (through sampling, for example) this problem can be solved in two different ways:\n\nScaling-out vertically, by using a machine with more available RAM. For some data scientists leveraging cloud environments like AWS, this can be as easy as changing the instance type of the machine (for example, AWS recently provided an instance with 2TB of RAM). However most companies today are using their internal data infrastructure that relies on commodity hardware to analyze data\u2014they\u2019ll have more difficulty increasing their available RAM.\nScaling-out horizontally: In this context, it is necessary to change the default R behaviour of loading all required data in memory and access the data differently by using a distributed or parallel schema with a divide-and-conquer (or in R terms, split-apply-combine) approach like MapReduce.\n\nWhile the first approach is obvious and can use the same code to deal with different data sizes, it can only scale to the memory limits of the machine being used. The second approach, by contrast, is more powerful but it is also more difficult to set up and adapt to existing legacy code.\nThere is a third approach\u2014scaling-out horizontally can be solved by using R as an interface to the most popular distributed paradigms:\n\nHadoop: through using the set of libraries or packages known as RHadoop. These R packages allow users to analyze data with Hadoop through R code. They consist on rhdfs to interact with HDFS systems; rhbase to connect with HBase; plyrmr to perform common data transformation operations over large datasets; rmr2 that provides a map-reduce API; and ravro that writes and reads avro files.\nSpark: with SparkR it is possible to use Spark\u2019s distributed computation engine to enable large-scale data analysis from the R shell. It provides a distributed data frame implementation that supports operations like selection, filtering, aggregation, etc., on large data sets.\nProgramming with Big Data in R: (pbdr) is based on MPI and can be used on high-performance computing (HPC) systems, providing a true parallel programming environment in R.\n\nNovel distributed platforms also combine batch and stream processing, providing a SQL-like expression language\u2014for instance, Apache Flink. There are also higher levels of abstraction that allow you to create a data processing language, such as the recently open sourced project Apache Beam from Google. However, these novel projects are still under development, and so far do not include R support.\nAfter the data preparation step, the next common data science phase consists of training machine learning models, which can also be performed on a single machine or distributed among different machines. In the case of distributed machine learning frameworks, the most popular approaches using R, are the following:\n\nSpark MLlib: through SparkR, some of the machine learning functionalities of Spark are exported in the R package. In particular, the following machine learning models are supported from R: generalized linear model (GLM), survival regression, naive Bayes and k-means.\nH2o framework: a Java-based framework that allows building scalable machine learning models in R or Python. It can run as standalone platform or with an existing Hadoop or Spark implementation. It provides a variety of supervised learning models, such as GLM, gradient boosting machine (GBM), deep learning, Distributed Random Forest, naive Bayes and unsupervised learning implementations like PCA and k-means.\n\nSidestepping the coding and customization issues of these approaches, you can seek out a commercial solution that uses R to access data on the front-end but uses its own big-data-native processing under the hood.\n\n\nTeradata Aster R is a massively parallel processing (MPP) analytic solution that facilitates the data preparation and modeling steps in a scalable way using R. It supports a variety of data sources (text, numerical, time series, graphs) and provides an R interface to Aster\u2019s data science library that scales by using a distributed/parallel environment, avoiding the technical complexities to the user. Teradata also has a partnership with Revolution Analytics (now Microsoft R) where users can execute R code inside of Teradata\u2019s platform.\nHP Vertica is similar to Aster, but it provides On-Line Analytical Processing (OLAP) optimized for large fact tables, whereas Teradata provides On-Line Transaction Processing (OLTP) or OLAP that can handle big volumes of data. To scale out R applications, HP Vertica relies on the open source project distributed R.\n\nOracle also includes an R interface in its advanced analytics solution, known as Oracle R Advanced Analytics for Hadoop (ORAAH), and it provides an interface to interact with HDFS and access to Spark MLlib algorithms.\n\nTeradata has also released an open source package in CRAN called toaster that allows users to compute, analyze, and visualize data with (on top of) the Teradata Aster database. It allows computing data in Aster by taking advantage of Aster distributed and parallel engines, and then creates visualizations of the results directly in R. For example, it allows users to execute K-Means or run several cross-validation iterations of a linear regression model in parallel.\nAlso related is MADlib, an open source library for scalable in-database analytics currently in incubator at Apache. There are other open source CRAN packages to deal with big data, such as biglm, bigpca, biganalytics, bigmemory or pbdR\u2014but they are focused on specific issues rather than addressing the data science pipeline in general.\nBig data analysis presents a lot of opportunities to extract hidden patterns when you are using the right algorithms and the underlying technology that will help to gather insights. Connecting new scales of data with familiar tools is a challenge, but tools like Aster R offer a way to combine the beauty and elegance of the R language within a distributed environment to allow processing data at scale.\nThis post is a collaboration between O'Reilly Media and Teradata. View our statement of editorial independence.\nContinue reading Scalable data science with R.",
  "title": "Scalable data science with R",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/tpl9nAArloM/scalable-data-science-with-r"
 },
 {
  "content": "Language Research, Probabilistic Cognition, Medical Data, and Teaching State Computer Security\n\nAI's Language Problem (Wired) -- very readable overview of the challenges and research in making more sense of the text we are processing, and why deep learning is a great start but not the end of the work to be done.\n\nProbabilistic Models of Cognition -- web book from Stanford profs behind webppl, a probabilistic programming language built on Javascript.\n\nYour Medical Data Misappropriated (BoingBoing) -- \"Property\" is a terrible framework for understanding personal information\u2014it's led to a situation where people aren't allowed to know what's going on in their own bodies, and where corporations can use anti-theft laws to attack scientists, security researchers, and the people whose bodies generated the data the corporations have turned into crown jewels.\n\n\nPwC Australia's Game to Teach Cyber Security Lessons (Computer Weekly) -- a friend has played this, says it is really good at conveying the complexity and the relentlessness (and is accessible for normal people).\n\nContinue reading Four short links: 10 August 2016.",
  "title": "Four short links: 10 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jv5twXgvBq4/four-short-links-10-august-2016"
 },
 {
  "content": "Decompose a sound into its harmonics, modify the harmonics, and generate new sounds.A signal represents a quantity that varies in time. That definition is pretty abstract, so let\u2019s start with a concrete example: sound. Sound is variation in air pressure. A sound signal represents variations in air pressure over time.\nA microphone is a device that measures these variations and generates an electrical signal that represents sound. A speaker is a device that takes an electrical signal and produces sound. Microphones and speakers are called transducers because they transduce, or convert, signals from one form to another.Continue reading DSP sounds and signals.",
  "title": "DSP sounds and signals",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/bXdBGUXP8jE/dsp-sounds-and-signals"
 },
 {
  "content": "Simplifying your application\u2019s log emission process.In Beyond the Twelve-Factor App, I present a new set of guidelines that builds on Heroku\u2019s original 12 factors and reflects today\u2019s best practices for building cloud-native applications. I have changed the order of some to indicate a deliberate sense of priority, and added factors such as telemetry, security, and the concept of \u201cAPI first\u201d that should be considerations for any application that will be running in the cloud. These new 15-factor guidelines are:\n\nOne codebase, one application\nAPI first\nDependency management\nDesign, build, release, and run\nConfiguration, credentials, and code\nLogs\nDisposability\nBacking services\nEnvironment parity\nAdministrative processes\nPort binding\nStateless processes\nConcurrency\nTelemetry\nAuthentication and authorization\n\nLogs (the 11th of the original 12 factors) should be treated as event streams, that is, logs are a sequence of events emitted from an application in time-ordered sequence. The key point about dealing with logs in a cloud-native fashion is, as the original 12 factors indicate, a truly cloud-native application never concerns itself with routing or storage of its output stream.Continue reading A cloud-native approach to logs.",
  "title": "A cloud-native approach to logs",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/flyYRBsYe-k/a-cloud-native-approach-to-logs"
 },
 {
  "content": "Learn about the surprising origin of the dark net, and find out how you can patrol this not-so-secret domain to detect and thwart intruders.\nIf you\u2019ve ever been burglarized, you know the drill: police officers arrive, they look briefly around your home, and then they ask you for a detailed list of the stolen items. In some cases, the stolen items are recovered within a few days and eventually returned.\nWhen cops find stolen goods quickly, it\u2019s most likely because they know where to look. Burglars aren\u2019t interested in keeping your flat-screen monitor and Xbox; they want cash. They bring their loot to a middleman (also known as a fence) who specializes in reselling stolen goods. Usually, the stolen goods sit in the fence\u2019s shed or basement until a buyer is found.Continue reading Patrolling the dark net.",
  "title": "Patrolling the dark net",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ogBon4gzPKQ/patrolling-the-dark-net"
 },
 {
  "content": "Explore the core components that make up a production XenServer deployment.\nCore Architecture and Critical Components\nIn not available, we stated that\u00a0\u201cXenServer is a pre-packaged, Xen-based virtualization solution.\u201d\u00a0This implies that anyone with sufficient skill can re-create XenServer by starting with the Xen hypervisor.\u00a0In reality, there is a rather large number of decisions anyone embarking on this task must make, and thankfully the team at Citrix has already made the bulk of those decisions.\u00a0In this chapter, we'll cover the core components that make up a production XenServer deployment.\nXenServer Isn\u2019t Linux, but dom0 Is\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\r\nThe misconception that XenServer is Linux is easily arrived at because from installation to privileged user space access, everything looks, feels, and tools much like a standard Linux environment. The boot loader used is extlinux, and the installer uses a familiar dialog for interactive setup and post installation. The administrator ends up within a Linux operating system logged in as the privileged user named root.Continue reading XenServer: Core architecture and critical components.",
  "title": "XenServer: Core architecture and critical components",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/nM30aMsseM8/xenserver-core-architecture-and-critical-components"
 },
 {
  "content": "Performance Engineering throughout the lifecycle.As you start to incorporate Performance Engineering capabilities into your lifecycle, it is important to understand what some of these areas are, and put these into context with some typical flow nomenclature. In the following sections we define each of these key elements\r\nwith specifics\u2014what, why, and how\u2014so you have a more complete understanding of how to add Performance Engineering throughout the lifecycle.\nOne of the challenges in building Effective Performance Engineering or a performance-first culture is defining who does what, when, and how. This kind of organizational alignment and agreement is as important as the daily scrum meeting of an Agile team. If everyone agrees that performance is important, but not on how to address it, then nothing is done about it.Continue reading A framework for performance .",
  "title": "A framework for performance",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/f5n5Jqp_weg/a-framework-for-performance"
 },
 {
  "content": "Learn how to configure and automate API caching for maximum performance and broad protection.Continue reading API caching: Extending app architecture performance and security.",
  "title": "API caching: Extending app architecture performance and security",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/c6M53TL7Pwg/api-caching-extending-app-architecture-performance-and-security"
 },
 {
  "content": "Systems with weak consistency guarantees can be expensive in unexpected ways.In 1983, Andreas Reuter and Theo H\u00e4rder coined the term ACID to describe the properties of a reliable transactional system. It stands for atomic, consistent, isolated, and durable. It\u2019s a great acronym.\nFast forward to present-day. Many developers have moved to distributed computing models with weaker consistency guarantees in exchange for that sweet, sweet speed. Not only is ACID generally considered to be too expensive for apps in terms of performance and availability, some think the term is now little more than marketing shellac. This is somewhat true\u2014very few systems that claim to support 100% ACID transactions actually do.\nBut is that a problem? Not every application benefits equally from strong ACID. While it\u2019s crucial in some\u2014especially in high-transaction speed industries like financial services\u2014a mostly-ACID system could work in others. However, many database implementers don\u2019t or can\u2019t know what \u201cmostly\u201d means for their operations.\nThis is especially true when it comes to isolation, which is where many systems that claim ACID transactions fall short. According to Peter Bailis, even though weak models like Read Committed Isolation or Snapshot Isolation represent \u201cACID in practice,\u201d it is difficult to know how they actually behave because they have not been thoroughly studied. The lack of information makes it hard to practice low-level isolation responsibly. To make matters worse, weak isolation can be an insidious problem, silently corrupting data until someone notices.\nThat\u2019s not to say that models like eventual consistency, which make no ACID guarantees, don\u2019t have their place. They were built to be fast and highly available during failures, and there they do excel. As a result, however, they burden application logic with handling temporary inconsistencies. The tradeoff might not be worth it, especially if you find out later on that weak consistency doesn\u2019t allow you to provide the kinds of guarantees that you need for your business (as was found in the case of Twitter\u2019s Manhattan).\nIn contrast, strongly transactional apps offer more straightforward code since atomicity and isolation allow the developer to skip building the code needed to handle partial failure and conflicting concurrent access. This can make a big difference. It can actually speed up apps by removing workarounds while at the same time shrinking the playing field for bugs and errors. It also reduces the need for distributed systems expertise at the user level, which was one of Google\u2019s criteria for building MillWheel.\nEngineers can do almost anything given enough time and resources. They can fix bugs, restore data, and retrofit systems that have fallen behind a business\u2019 needs. However, not everyone has a warehouse full of them waiting for a big distributed computing project. Not to mention that it\u2019s difficult to build full ACID transactions into a system if it wasn\u2019t a part of the original design.\nWhen building a system, it\u2019s important to consider that what seems easier or faster or cheaper at the beginning could end up being none of those things. The question isn\u2019t, \u201cDoes this work?\u201d but \u201cIf my app succeeds, will this level of consistency be a liability?\u201d And considering the option of ACID systems that do provide high levels of performance and availability, is \u201cACID-in-practice\u201d or eventual consistency good enough for your app?\nThis post is a collaboration between VoltDB and O\u2019Reilly. See our statement of editorial independence.\nContinue reading Why ACID transactions matter in an eventually consistent world.",
  "title": "Why ACID transactions matter in an eventually consistent world",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/2Ag-JgZb1po/why-acid-transactions-matter-in-an-eventually-consistent-world"
 },
 {
  "content": "Calvin Jia presents an in-depth overview of Alluxio and its role in the big data ecosystem. In this segment, he reviews examples that show how Alluxio complements Spark and S3, to enable fast data access.Continue reading Running Spark on Alluxio with S3.",
  "title": "Running Spark on Alluxio with S3",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/mUduqQKB7XA/running-spark-on-alluxio-with-s3"
 },
 {
  "content": "Forecasting Deadlines, SQL Client, Concept Network, and Game QA Process\n\nDon't Use Average to Forecast Deadlines -- good advice on how not to use throughput to predict delivery dates. [P]rojects will rarely have a distribution where average, median, and standard deviation are identical. As the data becomes skewed, the average loses its ability to provide the best central location for the data because the data will drag it away from the typical value. As we don\u2019t have information about the data distribution, it\u2019s wrong to make a forecast based on an average.\n\n\nSelectron -- GUI and command-line SQL client.\n\nConceptNet -- a semantic network containing a lot of things computers should know about the world, especially when understanding text written by people.\n\n\nPatch the Process -- games often release massive day-one patches because the console QA process (and the timeline to ship discs) means the devs have fixed things for months after the version that was approved for sale. This post explains that source of lag in more detail, and should make automated testing folks twitch.\n\nContinue reading Four short links: 9 August 2016.",
  "title": "Four short links: 9 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/gKr5Jutvmwc/four-short-links-9-august-2016"
 },
 {
  "content": "Venture capital firms with multimillion-dollar portfolios have recently begun to add design partners to work with startups during their formative stages. This report describes what this emerging role means to designers, investors, and entrepreneurs. \nIntroduction\n\u201cHow can I get a job like yours?\u201d \u201cHow do you spend your time?\u201d \u201cWhat does a design partner in a venture capital firm do?\u201d The frequency of these questions posed to me since joining Khosla Ventures two years ago prompted me to write this report. The role of design partner inside venture capital (VC) firms is still relatively new and undefined, with a little more than a handful of people holding this job title on Sand Hill Road. That prominent VC firms with multimillion-dollar portfolios would want to include designers as partners strikes many as a validating sign that design has finally arrived: entrepreneurs and investors value design so much that they want designers involved during the most formative stages of a company\u2019s development. So, it\u2019s no surprise that there would be considerable curiosity, interest, and excitement about this role as a possible career choice for designers. \nMy goals in writing this report are threefold: Continue reading Design in venture capital.",
  "title": "Design in venture capital",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/PMF4-7HYzvI/design-in-venture-capital"
 },
 {
  "content": "Explore Lego Serious Play, a proven tool for boosting both individual and team productivity. \nBuild to Lead: How Lego Bricks Can Make You a Better Leader\r\n\nHarnessing the Power of Play at Work\nWhat if you could harness the power of play\u2014something we all knew but most of us forgot\u2014to empower your teams, and at the same time help you realize creative and powerful solutions in the face of today\u2019s business challenges? There is a tried-and-true process\u2014Lego Serious Play\u2014that is guaranteed to expand your leadership capacity and deliver predictable and consistently productive results. You will learn how and why this tool boosts both individual and team productivity. It sounds almost too good to be true, but, yes, playing with Lego bricks can help make you and your team more productive (see Figure\u00a01-1). And who doesn\u2019t love an excuse to play with Lego bricks?\n\nFigure 1-1. Businesses face a number of challenges\n\nLego Serious Play is a facilitated team-thinking and problem-solving process in which you build Lego models in response to challenge questions, such as \u201cBuild a barrier to teamwork.\u201d The process has a variety of applications that can be used for problem solving, strategy development, feedback, ideation, product development, relationship building, goal setting, debriefing, and performance reviews. And the 3D representations create an easy to understand, level playing field where everybody has a voice and everybody can express his or her thoughts. It\u2019s an incredibly effective way to get everyone\u2019s ideas on the table and, together, develop a collective plan of action (see Figure\u00a01-2).Continue reading Build to lead: How Lego bricks can make you a better leader.",
  "title": "Build to lead: How Lego bricks can make you a better leader",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/oumxp_0I0A0/build-to-lead--how-lego-bricks-can-make-you-a-better-leader"
 },
 {
  "content": "Star Simpson, Inside Malware Network, Code Generation, and Trippy Face Tricks\n\nInterview with Star Simpson (BoingBoing) -- Foo and my personal superhero on the Cool Tools podcast.\n\nWire Wire: A West African Cyber Threat -- fascinating look inside the malware Nigerian scammers use. Amusingly, the security researchers were able to study what the scammers do because the scammers inadvertently installed their malware on their own boxes, so their keystrokes were logged to command-and-control machines. Dog-fooding your malware is just not best practice, guys. (via BoingBoing)\n\nLatent Predictor Networks for Code Generation (PDF) -- Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalizing multiple predictors allows our model to outperform strong benchmarks.\n\n\nOmote Demonstration (YouTube) -- stunning real-time face tracking and projection mapping system.  Starts out as makeup and then gets so much more wow. Demo two is where the acid kicks in. (via BoingBoing)\n\nContinue reading Four short links: 8 August 2016.",
  "title": "Four short links: 8 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/TO2Q3L1GP6M/four-short-links-8-august-2016"
 },
 {
  "content": "How combining data and applying time-series techniques can provide insights into a company\u2019s operational strengths and weaknesses.Many businesses are turning to data analytics to provide insight for making operational decisions. Two areas in particular where data analytics can help companies is (1) improved service delivery to customers, and (2) more efficient and effective resource allocation. To arrive at actionable insights, the analysis often relies on multiple data sets of varying size and content. In this article, we will discuss one simple example where data engineering, data analysis, and the merging of two data sets can help a company in both the above areas.\nUse case: Scheduling 24-hour support staff\nMany companies provide ongoing support for their products and services. This support often requires a round-the-clock team of support technicians and engineers to quickly respond to issues as they arise. Depending on how customers use the product and their geographic locations, however, it may be difficult to appropriately schedule support staff across the 24-hour period, leading to the possibility of overstaffing or understaffing during any given period. Making appropriate support-staffing decisions, however, speaks to both critical operations areas noted above. Understaffing can lead to delayed response times when issues arise, reducing the quality of customer service. Overstaffing indicates that resources are being underutilized, adding unnecessary costs.\nA straightforward approach to making staffing decisions might involve estimating a couple of metrics: (1) the baseline productivity of a single staff member (e.g. how many tickets can a staff member respond to in a given period of time), and (2) any temporal patterns to when tickets are generated (e.g., is the ticket generation rate different by hour of day or day of week). But these metrics can be difficult to determine without complex data analysis, and using them in a straightforward way may also require making several simplifications and assumptions. An alternative approach is to examine patterns in the metric of interest and make scheduling adjustments based on that metric.\nOur approach to measuring responsiveness\nIn a round-the-clock customer support example, a good metric for support staff responsiveness is the amount of time it takes for a support technician to take a first action in response to a service ticket. In this scenario, we are imagining that issues are raised through a software interface that generates a service ticket and that the entire service team has the ability to respond to tickets in the service queue. The goal of the analysis, then, is to understand how this metric varies by staffing level and determine if any adjustments need to be made.\nPreparing the data\nFor a software ticketing system like the one described above, service tickets may be stored in a historical time series that produces a record each time an action related to the ticket is taken. If the data are stored in a relational system, these historical records may also connect to metadata related to the ticket, such as the entity that opened the ticket, and further details about the ticket.\nRegardless of the complexity of the database, however, it should be possible to join and query the database system to obtain a single time series table where each record contains the following information: ticket ID number, time of action, and action taken.\u00a0 Depending on the specific problem, there may be important metadata that should be included to further segment the data, such as the type of ticket; but for this example, we will assume the simplest case, where all ticket types can be treated the same.\nThe data table stub below shows a sample of what such a time series table might look like.\n\n\n\n\nTicket ID\n\n\nTime\n\n\nAction\n\n\n\n\nTKT101\n\n\nApril 4, 2016 01:03PM GMT\n\n\nCreated\n\n\n\n\nTKT101\n\n\nApril 4, 2016 01:06PM GMT\n\n\nModified by Technician\n\n\n\n\nTKT102\n\n\nApril 4, 2016 01:13PM GMT\n\n\nCreated\n\n\n\n\nTKT103\n\n\nApril 4, 2016 01:14PM GMT\n\n\nCreated\n\n\n\n\nTKT102\n\n\nApril 4, 2016 01:17PM GMT\n\n\nModified by Technician\n\n\n\n\nTKT104\n\n\nApril 4, 2016 01:17PM GMT\n\n\nCreated\n\n\n\n\nTKT104\n\n\nApril 4, 2016 01:21PM GMT\n\n\nModified by Technician\n\n\n\n\nTKT105\n\n\nApril 4, 2016 01:22PM GMT\n\n\nCreated\n\n\n\n\nTKT106\n\n\nApril 4, 2016 01:22PM GMT\n\n\nCreated\n\n\n\n\nUsing these data, we can derive another data set that gives the amount of time elapsed between when a ticket is created and the time of first action. These derived data (sample shown below) will serve as the basis for our analysis.\n\n\n\n\nTicket ID\n\n\nTime Created\n\n\nTime to First Action (Min)\n\n\n\n\nTKT101\n\n\nApril 4, 2016 01:03PM GMT\n\n\n3\n\n\n\n\nTKT102\n\n\nApril 4, 2016 01:13PM GMT\n\n\n4\n\n\n\n\nTKT103\n\n\nApril 4, 2016 01:14PM GMT\n\n\n10\n\n\n\n\nTKT104\n\n\nApril 4, 2016 01:17PM GMT\n\n\n4\n\n\n\n\nTKT105\n\n\nApril 4, 2016 01:22PM GMT\n\n\n5\n\n\n\n\nTKT106\n\n\nApril 4, 2016 01:22PM GMT\n\n\n6\n\n\n\n\nAggregating the data\nWe can aggregate the data in several ways to obtain useful insights into support operations. To answer the initial question we posed about whether support staffing levels are adequate, we would aggregate the data by determining the average time to first action for tickets created during each hour of day.\u00a0 Since staffing schedules may change periodically, often on a monthly basis, we also limit the analysis to tickets created during the specific period of time when a particular schedule was in effect.\n\nFigure 1. Chart courtesy of Arti Garg.\n\nThis plot shows the average initial response time, by hour of day that a ticket was created (in red). For comparison, we also show the average number of tasks opened, by hour of day (in blue). The black dashed line shows the number of support staff working during each hour of day. Perhaps the most striking feature of this plot is that large increase in response time for tickets opened between 16h to 19h (4 p.m. to 7 p.m.); this increase also coincides with a drop in staffing levels from three people to one person.\nThe immediate implication, based on a qualitative visual examination of the above chart, alone, is that staffing levels should be increased during the 16h to 19h period. It is interesting, however, to examine this a bit more quantitatively.\nInitial response time vs. ticket creation rate\nWe might expect there to be a relationship between the average initial response time and the average numbers of tasks that are generated at any particular time. The plot below shows, for each hour of day, the average response time compared to the average number of tickets created (indicated as green dots).\nThe black dashed line shows a linear fit to these data, used to determine if there is a trend in this relationship. There is also a significant outlier corresponding to the 18h to 19h period, with an unusually high response time. If we remove this point when performing the linear fit, we obtain the trend shown in the solid black line. Comparing the RMSE calculated, after removing the outlying point for either fit, does not indicate a substantial difference (1.4 versus 1.5), and it\u2019s also apparent by visual inspection that neither fit provides much predictive value.\u00a0\nFor example, based on the fit alone, we might expect that at no ticket creation rate below 4/hr should we expect the initial response time to exceed five minutes, but the data clearly show five hours of day when the response time is greater than five minutes.\n\nFigure 2. Chart courtesy of Arti Garg.\n\nInitial response time vs. staffing level\nAnother relationship we should examine is between the staffing level and the average initial response time. The plot below indicates a much more apparent trend where the higher the staffing level, the shorter the initial response time. For each hour-of-day, the blue dots show the average response time for any given staffing level. Once again, we perform a simple linear fit to the data with (dashed black line) and without (solid black line) the outlier corresponding to the 18h to 19h block.\n\nFigure 3. Chart courtesy of Arti Garg.\n\nIn this case, removing the outlier does appear to have a more significant impact on the trendline, though the predictive value of both fits are similar. To use the example above, both fits would suggest that keeping the minimum staff level at three people or more would result in an average initial response time below five minutes. There are only two hours of day where these models are incorrect (11h to 12h and 13h to 14h), and in both of those cases, the average response time is still below six minutes.\nGiven this trend, the longer initial response time during the 18h to 19h block is less surprising, though this hour remains a significant outlier. To better understand this, we can go back to our initial plot, which showed the data as a time series. When we do so, it becomes obvious that the increase in initial response time occurs during the final hour of a three-hour block, where there is only one person staffed. Analysis of additional data concerning the other duties the support staff are attending to may offer better insight into this outlier. Initial hypotheses, though, could be that either the staff member on duty begins to feel fatigue during her third hour alone, slowing down her overall performance or she develops a backlog of work from her competing responsibilities, which slows down her initial response time.\nThe analyses we have already discussed above, however, suggest that to make an improved staffing decision, the support operations manager does not need to understand the cause for the 18h to 19h block outlier. Increasing the staffing level to two or three people during this period is likely to reduce the overall initial response time. If the manager sets an initial response time target of around five minutes, these analyses also suggest that the team is overstaffed during the 1h to 8h block, when there are four or five staff members on duty. Rescheduling these staff to later parts of the day will likely reduce the average initial response time overall, and significantly improve the response time during the 16h to 19h block.\nMaking better decisions\nData analytics offer powerful tools for helping a company make better operations decisions. In particular, combining data from multiple data sources and applying time-series techniques can provide deep insights into a company\u2019s operational strengths and weaknesses. In this example, we have shown how combining straightforward analyses of a company\u2019s ticket management data with information about its staffing scheduling can help a support operations manager make better staff scheduling decisions.\nThis example was put together by Datapipe\u2019s data and analytics team.\nContinue reading Improving operations using data analytics.",
  "title": "Improving operations using data analytics",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/4FogqY_feiI/improving-operations-using-data-analytics"
 },
 {
  "content": "Challenge Winner, Rewriting Rules, Games for Ground Truth, and Fast Text Processing\n\nNIH Pill Image Recognition Challenge -- congrats to Foo Greg Sadetsky, whose team took 2nd place!\n\nTime to Rewrite The Rules (Tim O'Reilly) -- You can see here that there is a five-player game in which gains (or losses) can be allocated in different proportion to consumers, the company itself, financial markets, workers, or taxpayers. The current rules of our economy have encouraged the allocation of gains to consumers and financial shareholders (now including top company management), and the losses to workers and taxpayers. But it doesn\u2019t have to be that way.\n\n\nGround Truth from Computer Games -- clever hack: using photorealistic games to train image segmenting algorithms.\n\nfastText -- a library for efficient learning of word representations and sentence classification. C++, from Facebook, BSD-licensed.\n\nContinue reading Four short links: 5 August 2016.",
  "title": "Four short links: 5 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/JZDB-_zmt5s/four-short-links-5-august-2016"
 },
 {
  "content": "The top 5 habits of a professional data scientist.There are people who can imagine ways of using data to improve an enterprise. These people can explain the vision, make it real, and affect change in their organizations. They are\u2014or at least strive to be\u2014as comfortable talking to an executive as they are typing and tinkering with code. We sometimes call them \u201cunicorns\u201d because the combination of skills they have are supposedly mystical, magical\u2026and imaginary.\nBut I don\u2019t think it\u2019s unusual to meet someone who wants their work to have a real impact on real people. Nor do I think there is anything magical about learning data science skills. You can pick up the basics of machine learning in about 15 hours of lectures and videos. You can become reasonably good at most things with about 20 hours (45 minutes a day for a month) of focused, deliberate practice.\nSo, basically, being a unicorn, or rather a professional data scientist is something that can be taught. Learning all of the related skills is difficult, but straight-forward. With help from the folks at O\u2019Reilly, we\u2019ve designed a tutorial at Strata + Hadoop World New York, 2016, Data Science that Works: Best practices for designing data-driven improvements, making them real, and driving change in your enterprise, for those who aspire to the skills of a unicorn. The premise of the tutorial is that you can follow a direct path toward professional data science, by taking on the following, most distinguishing habits:\n5. Put aside the technology stack\nThe tools and technologies used in data science are often presented as a technology stack. The stack is a problem because it encourages you to to be motivated by technology, rather than business problems. When you focus on a technology stack, you ask questions like \u201ccan this tool connect with that tool\u201d or \u201cwhat hardware do I need to install this product?\u201d These are important concerns, but they aren\u2019t the kinds of things that motivate a professional data scientist.\nProfessionals in data science tend to think of tools and technologies as part of an insight utility, rather than a technology stack. Focusing on building a utility forces you to select components based on the insights that the utility is meant to generate. With utility thinking, you ask questions like \u201cWhat do I need to discover an insight?\u201d and \u201cWill this technology get me closer to my business goals?\u201d\n\nFigure 1. Data science tools and technologies as components of an insight utility, rather than a technology stack. Credit: Jerry Overton.\n\nIn the Strata + Hadoop World tutorial in New York, I\u2019ll teach simple strategies for shifting from technology-stack thinking to insight-utility thinking.\n4. Keep data lying around\nData science stories are often told in the reverse order from which they happen. In a well-written story, the author starts with an important question, walks you through the data gathered to answer the question, describes the experiments run, and presents resulting conclusions. In real data science, the process usually starts when someone looks at data they already have and asks: \u201chey, I wonder if we could be doing something cool with this?\u201d That question leads to tinkering, which leads to building something useful, which leads to the search for someone who might benefit. Most of the work is devoted to bridging the gap between the insight discovered and the stakeholder\u2019s needs. But when the story is told, the reader is taken on a smooth progression from stakeholder to insight.\nThe questions you ask are usually the ones where you have access to enough data to answer. Real data science usually requires a healthy stockpile of discretionary data. In the tutorial, I\u2019ll teach techniques for building and using data pipelines to make sure you always have enough data to do something useful.\n3. Have a strategy\nData strategy gets confused with data governance. When I think of strategy, I think of chess. To play a game of chess, you have to know the rules. To win a game of chess, you have to have a strategy. Knowing that \u201cthe D2 pawn can move to D3 unless there is an obstruction at D3 or the move exposes the king to direct attack\u201d is necessary to play the game, but it doesn\u2019t help me pick a winning move. What I really need are patterns that put me in a better position to win\u2014\u201cIf I can get my knight and queen connected in the center of the board, I can force my opponent\u2019s king into a trap in the corner.\u201d\n\nFigure 2. A data strategy map. Data strategy is not the same as data governance. To execute a data strategy, you need a map. Credit: Jerry Overton.\n\nThis lesson from chess applies to winning with data. Professional data scientists understand that to win with data, you need a strategy; and to build a strategy, you need a map. In the tutorial, we\u2019ll review ways to build maps from the most important business questions, build data strategies, and execute the strategy using utility thinking.\n2. Hack\nBy hacking, of course, I don\u2019t mean subversive or illicit activities. I mean cobbling together useful solutions. Professional data scientists constantly need to build things quickly. Tools can make you more productive, but tools alone won\u2019t bring your productivity to anywhere near what you\u2019ll need.\nTo operate on the level of a professional data scientist, you have to master the art of the hack. You need to get good at producing new, minimum-viable, data products based on adaptations of assets you already have. In New York, we\u2019ll walk-through techniques for hacking together data products and building solutions that you understand, and are fit for purpose.\n1. Experiment\nI don\u2019t mean experimenting as simply trying out different things and seeing what happens. I mean the more formal experimentation prescribed by the scientific method. Remember those experiments you performed, wrote reports about, and presented in grammar-school science class? It\u2019s like that.\nRunning experiments and evaluating the results is one of the most effective ways of making an impact as data scientist. I\u2019ve found that great stories and great graphics are not enough to convince others to adopt new approaches in the enterprise. The only thing I\u2019ve found to be consistently powerful enough to affect change is a successful example. Few are willing to try new approaches until it has been proven successful. You can\u2019t prove an approach successful unless you get people to try it. The way out of this vicious cycle is to run a series of small experiments.\n\nFigure 3. Small continuous experimentation is one of the most powerful ways for a data scientist to affect change. Credit: Jerry Overton.\n\nIn the tutorial at Strata + Hadoop World New York, we\u2019ll also study techniques for running experiments in very short sprints, which forces us to focus on discovering insights and making improvements to the enterprise in small, meaningful chunks.\nWe\u2019re at the beginning of a new phase of big data\u2014a phase that has less to do with the technical details of massive data capture and storage, and much more to do with producing impactful scalable insights. Organizations that adapt, and learn to put data to good use, will consistently outperform their peers. There is a great need for people who can imagine data-driven improvements, make them real, and drive change. I have no idea how many people are actually interested in taking on the challenge, but I\u2019m really looking forward to finding out.\nContinue reading There\u2019s nothing magical about learning data science.",
  "title": "There\u2019s nothing magical about learning data science",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/QEaPWggR5fE/theres-nothing-magical-about-learning-data-science"
 },
 {
  "content": "10 a.m. PT September 22, 2016. Join UX strategist and business consultant Heather O\u2019Neill, CEO of Pixels for Humans, for a hands-on, in-depth exploration of creating and improving designs using data.\r\nContinue reading O'Reilly Online Training: Data-Driven UX Design.",
  "title": "O'Reilly Online Training: Data-Driven UX Design",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/QHZqVFeIwRg/oreilly-online-training-data-driven-ux-design"
 },
 {
  "content": "Our current economic rules encourage the allocation of gains to consumers and financial shareholders, and the losses to workers and taxpayers. It doesn\u2019t have to be that way.Modern economics likes to think of itself as a science, and too often, its practitioners have attempted to uncover its \"laws,\" as if they were modern Isaac Newtons uncovering the laws of motion. But many of the laws of economics are far more like the rules of a game than like the laws of nature. Some of the rules represent what appear to be fundamental constraints\u2014the availability of resources, say, or the absorptive capacity of the environment, or even the behavioral patterns of human nature\u2014while others are arbitrary and subject to change, such as tax policy, government entitlements, and minimum wage requirements.\nAn economy has untold possible outcomes. Its complexity comes both from the near infinite variety that can come from permutations of simple rules, and from the fact that billions of humans are playing the game simultaneously, each affecting the outcomes for each other. Many of the rules are written down nowhere, controlled by no one, and constantly evolving. Individuals, businesses, and governments are all players, and none of them can know the full consequences of their decisions.\nEven the simplest and most definitive of the \u201crules\u201d of an economy are far more complex to apply than they appear on paper. As an Internet wag noted many years ago, \"The difference between theory and practice is always greater in practice than it is in theory.\" This difference between theory and practice is driven by complex interactions, not only between rules but between multiple players with competing incentives.\nThis complexity came to mind last year in a conversation I had with Uber's economists. I was arguing that just as Google's search algorithm takes many factors into account in producing the \"best\" results, Uber's algorithm would benefit if it took drivers' wages, job satisfaction, and turnover into account, and not just passenger pickup time, which is its current fitness function. (Uber aims to have enough drivers on the road in a given location that the average pickup time is no more than three minutes.)\nThe economists explained to me that Uber's wages were, by definition, optimal, because they simply represent a demand curve, one of the most basic laws of economics.\n\nFigure 1. via\u00a0Wikispaces\n\nUber\u2019s real-time matching algorithm satisfies two overlapping demand curves. If there are not enough passengers, the price must go down to stimulate passenger demand. That's the essence of Uber's frequent price cuts. But if there are not enough drivers to satisfy that demand, the price has to go up to encourage more drivers to come on the road. That's the essence of surge pricing.\nUber\u2019s argument is that the algorithmically determined cost of a ride is at the sweet spot that will drive the most passenger demand while also providing sufficient incentive to produce the number of drivers to meet that demand. And because driver income is the product of both the number of trips and the rate paid, that sweet spot will also maximize driver income. Any attempt to set rates to specifically raise driver income would suppress rider demand, and so reduce utilization, and thus wages. Of course, if too many drivers show up, this will also reduce utilization, but the economists seem confident, based on data that they were not authorized to share with me, that they have generally found that sweet spot.\nIf Uber had the courage of its convictions, it would be doing completely algorithmic pricing (including surging prices in a negative direction, below the base price), much as Google sets ad prices with an auction. Why don\u2019t they? Because they believe that customers are more comfortable with a known base price. That is, the difference between theory and practice is greater in practice than it is in theory.\nI do believe that labor marketplace algorithms can be game changers for business and society if they are used to model and satisfy more and more complex conditions. There\u2019s no question that even in their current state, Uber's real-time marketplace algorithms allow for far better matching of supply and demand than the previous structure of the taxicab and limousine industry. But Uber can do better. Algorithms such as these can be a real advance in the structure of our economy, but only if they take into account the needs of workers as well as those of consumers, businesses, and investors.\nHere's the rub in the real world: Uber isn\u2019t just satisfying the two simultaneous demand curves of customer and driver needs, but also competitive business needs. Their desire to crush the incumbent taxi industry and to compete with rivals like Lyft in the U.S. market and Didi in China also affects their pricing. Under the rules of the venture-backed startup game, they must grow at a rate that will allow them to utterly dominate the new industry that they have created in order to satisfy the enormous valuation placed on them by their investors.\nDrivers are also not playing a simple game in which they can just go home if the wages aren't sufficient. They have bills to pay, and may have to work far more hours than they would like in order to meet them. They may know in theory that they are depreciating the value of their vehicle and running up expenses that undermine their hourly earnings, but they don't feel they have any choice. Alternative jobs may be even worse, with less flexibility and even lower pay.\nI suspect that over time, driver wages will need to increase at some rate that is independent of the simple supply and demand curves that characterize Uber's algorithm today. Even if there are enough drivers, the quality of drivers deeply influences the customer experience.\nDriver turnover is a key metric. As long as there are a lot of people willing to try working for the service, it is possible to treat drivers as a disposable commodity. But this is short-term thinking. What you want are drivers who love the job and are good at it, are paid well, and as a result, keep at it. Over the long term, I predict that Uber and Lyft will be engaged in as fierce a contest to attract and keep drivers as they are to attract and keep customers today. And that competition may well provide further evidence that higher wages can pay for themselves by inducing productivity and greater consumer satisfaction.\nMany simplistic apologists for the capitalist system celebrate disruption and assume that while messy, it will all work out for the best if we just let \"the invisible hand\" do its work. This is true, if we correctly understand the invisible hand. The law of supply and demand is not describing some invisible force, but the way that players of the game fight for competitive advantage. There are games within games. As Adam Smith put it:\nIt is not from the benevolence of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages.\nThe \"law\" emerges from the contest between players. As labor organizer David Rolf said to me, \"God did not make being an auto worker a good job!\" Those middle class jobs that we look back at with such nostalgia were the result of a fierce competition between companies and labor as to who would set the rules of the game. The invisible hand became very visible indeed by way of bitter strikes, and then transcended the market into the political process with the National Labor Relations Act of 1935 (the Wagner Act), the Labor Management Relations Act of 1947 (Taft-Hartley), and state \u201cright to work laws.\u201d Over the past 80 years, these acts have tilted the rules first one way, then the other.\nRight now, we're at an inflection point, where many rules are being profoundly rewritten. Much as happened during the industrial revolution, new technology is obsoleting whole classes of employment while making untold new wonders possible. It is making some people very rich, and others much poorer. It is giving companies new ways to organize; those new forms of organization are gradually being matched by labor.\nI am confident that the invisible hand will do its work. But not without a lot of struggle. The political convulsions we've seen in the U.K. and now in the U.S. are a testament to the difficulties we face if we let the invisible hand struggle through normal channels! We are heading into a very risky time.\nThese discussions are more than theoretical. Rising global inequality is triggering a political backlash that could lead to profound destabilization of both society and the economy. Alas, as my friend Bill Janeway wrote to me in an email, \u201cThe supposed laws of welfare economics assert that the optimal distribution of wealth is achieved when (1) no one can be made better off if done so by making someone worse off and (2) the winners compensate the losers. It is also rarely that such compensation is rendered 'from the benevolence' of the winners! Unfortunately, the winners rarely do, except as the result of political coercion.\u201d That political coercion may be at hand.\nMany discussions of our technological future assume that the fruits of productivity will be distributed to the benefit of all. And that is clearly not the case. Right now, the economic game is enormously fun for far too few players, and an increasingly miserable experience for many others.\n\u201cBetween the end of World War II and 1968, the minimum wage tracked average productivity growth fairly closely,\u201d wrote economist John Schmitt. \u201cSince 1968, however, productivity growth has far outpaced the minimum wage. If the minimum wage had continued to move with average productivity after 1968, it would have reached $21.72 per hour in 2012\u2014a rate well above the average production worker wage. If minimum-wage workers received only half of the productivity gains over the period, the federal minimum would be $15.34.\u201d\nMeanwhile, the vast bulk of the value created by increasing productivity has been allocated to corporate profits. Contrary to what you might expect, this is not because companies need those higher profits to grow and sustain themselves, investing in new products and hiring more people in the process. It is because of the unintended consequences of rules designed to align the interests of management and shareholders that instead made management prioritize growth of the stock price above all other considerations. As Rana Foroohar, author of the book Makers and Takers and one of the speakers at this year\u2019s Next:Economy Summit, put it in a recent Time magazine cover story, \u201cthe single biggest unexplored reason for long-term slower growth is that the financial system has stopped serving the real economy and now serves mainly itself.\u201d\nAnother huge swath of value has been allocated to consumer surplus\u2014the difference between what goods sell for and what customers might have been willing to pay. (A huge amount of the value that new technology brings has been provided to consumers free of charge, creating consumer surplus that is difficult to measure.) Free trade and depressed wages have also led to fierce competition by companies to expand their market share by offering goods at lower prices (much as Uber has done with taxi fares.) This is also a powerful kind of consumer surplus, and one of many strategies that economic game players employ to gain advantage.\nI like to use Walmart as an example of the complexity of the game play and the tradeoffs that players ask us to make as a society. Walmart has built an enormously productive business that has vastly reduced the cost of the goods that it supplies. A large part of the value goes to consumers in the form of lower prices. Another large part goes to corporate profits, which benefits both company management and outside shareholders. But meanwhile, Walmart workers are paid so little that most need government assistance to live\u2014by coincidence, the difference between Walmart wages and a $15 minimum wage for their U.S. workers (approximately $5 billion/year) is not that far off from the $6 billion/year that Walmart workers are subsidized via Federal supplemental nutrition assistance (SNAP, formerly known as \u201cfood stamps\u201d).\nYou can see here that there is a five-player game in which gains (or losses) can be allocated in different proportion to consumers, the company itself, financial markets, workers, or taxpayers. The current rules of our economy have encouraged the allocation of gains to consumers and financial shareholders (now including top company management), and the losses to workers and taxpayers. But it doesn\u2019t have to be that way.\nWe can wait for the invisible hand (i.e., the push and pull of the many players in the game) to work things out, or we can try out different strategies for getting to optimal outcomes more quickly. We can rewrite the rules.\nIn professional sports, leagues concerned about competitive play often establish new rules. Football (soccer) has changed its rules many times over the past 150 years. NBA basketball added the 3-point shot in 1979 to make the game more dynamic; rule changes are being proposed again after the game-changing play of Golden State Warriors star Stephen Curry. Many sports use salary caps to keep teams in large markets from buying up the best talent and making it impossible for smaller markets to compete. And so on.\nThe \u201cfight for 15,\u201d the movement toward a national $15 minimum wage, is one way to rewrite the rules. Businesses and free market fundamentalists argue that raising minimum wages will simply cause businesses to eliminate jobs, making workers even worse off. The evidence shows that this isn\u2019t the case. As Nick Hanauer said during the Q&A at last year\u2019s Next:Economy Summit, \u201cThat\u2019s an intimidation tactic masquerading as an economic theory.\u201d\nThe key question, expressed in the true language of Adam Smith\u2019s \u201cinvisible hand,\u201d is who gets more, and who gets less. Capital, labor, consumers, taxpayers.\nAs noted above, a $15 minimum wage might cost Walmart on the order of $5 billion/year. This is no small number. It represents about a quarter of Walmart\u2019s annual profits, and about 1.25% of its annual U.S. revenues. But it might save taxpayers $6 billion per year (and that\u2019s just the amount used to subsidize Walmart; including all the other low-wage employers in America, the number is far larger.)\nIf Walmart weren\u2019t able to pass off part of its true costs onto taxpayers, the company would have to cut its profits or raise its prices. But is that really such a bad thing? Let\u2019s do some back-of-the-napkin math. If Walmart were to reduce its profits by $5 billion (approximately 20%), its market cap might fall, a loss to shareholders. But leaving aside the shock of a sudden drop in earnings due to a change in the rules, would the owners of Walmart really not have wanted to own it if it generated $20 billion a year in profit instead of $25 billion?\nIf Walmart were to pass along the additional costs to consumers, prices would have to go up by 1.25% (or $1.25 for every $100 spent at Walmart). If the costs were split between capital and consumers, that would require only a 10% drop in Walmart profits and an additional 62 cents per $100 spent by consumers. Would people really stop shopping at Walmart if they had to spend little more than an additional half cent for every dollar?\nThose higher prices might discourage some customers, but the higher incomes of workers might encourage them to spend more. So, it\u2019s not inconceivable that Walmart and its shareholders would come out whole.\nAnd of course, raising the minimum wage is only one way to address the way that the current rules of our economy favor owners of capital over human workers. Tax rates really do need some rethinking! Why do we have preferential rates for taxes on capital when it is so abundant that much of it is sitting on the sidelines rather than at work in our economy? Why do we tax labor income when one of the problems in our economy is lack of aggregate demand due to insufficient consumer spending?\nWe could change these relative tax rates, and even institute a \u201cWealth tax\u201d such as proposed by Thomas Piketty and use the proceeds to help fund a Universal Basic Income! In fact, why not tax carbon rather than labor, substituting a carbon tax for social security taxes, among the most regressive of all taxes imposed? These rule changes might be even more costly to capital owners but might well benefit society overall.\nThese are political decisions as much as they are purely economic or business decisions. And that is appropriate. Economic policy shapes the future not just for one person or one company, but for all of us.\nThroughout history and across continents, economies have played the game using different rules: all land belongs to kings and aristocrats. No one can own the land. All property should be held in common. Property should be private. Property is entailed and cannot be sold by the owners or heirs. Labor belongs to kings and aristocrats and must be supplied on demand. A man's labor is his own. Women belong to men. Women are independent economic actors. Children are a great source of cheap labor. Child labor is a violation of human rights. Humans can be the property of other humans. No human can be enslaved by another.\nWe look back at some of these rules as barbaric, and others as utopian dreams. But we also can see that some rules have led to golden periods when society flourished.\nHere is one of the failed rules of today\u2019s economy: humans are expendable. Their labor should be eliminated as a cost whenever possible. This will increase the profits of a business, and richly reward investors. These profits will trickle down to the rest of society.\nThe evidence is in. This rule doesn\u2019t work.\nIt\u2019s time to rewrite the rules. We need to play the game of business as if people matter.\nContinue reading The game of business: It's time to rewrite the rules.",
  "title": "The game of business: It's time to rewrite the rules",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/YpJMQoylZg4/the-game-of-business-its-time-to-rewrite-the-rules"
 },
 {
  "content": "The O\u2019Reilly Design Podcast: AI, understanding algorithms, and design diversity.In this week\u2019s Design Podcast, I sit down with Giles Colborne, designer, author, and managing director of cxpartners. We talk about how AI is reinventing design and the roles of designers; the balance of creating something that is different but familiar; and how, at its most basic level, AI is shortcutting user input.Continue reading Giles Colborne on how AI is reinventing design.",
  "title": "Giles Colborne on how AI is reinventing design",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vcDBwvr5WBg/giles-colborne-on-how-ai-is-reinventing-design"
 },
 {
  "content": "Near-real-time processing yields increased efficiency and an opportunity for unified architecture.Uber\u2019s mission is to provide \u201ctransportation as reliable as running water, everywhere, for everyone.\u201d To fulfill this promise, Uber relies on making data-driven decisions at every level, and most of these decisions can benefit from faster data processing. For example, using data to understand areas for growth or accessing of fresh data by the city operations team to debug each city. Needless to say, the choice of data processing systems and the necessary SLAs are the topics of daily conversations between the data team and the users at Uber.\nIn this post, I would like to discuss the choices of data processing systems for near-real-time use cases, based on experiences building data infrastructure at Uber as well as drawing from previous experiences. In this post, I argue that by adding new incremental processing primitives to existing Hadoop technologies, we will be able to solve a lot more problems, at reduced cost, and in a unified manner. At Uber, we are building our systems to tackle the problems outlined here and are open to collaborating with like-minded organizations interested in this space.\nNear-real-time use cases\nFirst, let\u2019s establish the kinds of use cases we are talking about: cases in which up to one-hour latency is tolerable are well understood and mostly can be executed using traditional batch processing via MapReduce/Spark, coupled with incremental ingestion of data into Hadoop/S3. On the other extreme, cases needing less than one to two seconds of latency typically involve pumping your data into a scale-out key value store (having worked on one at scale) and querying that. Stream processing systems like Storm, Spark Streaming, and Flink have carved out a niche of operating really well at practical latencies of around one to five minutes, and are needed for things like fraud detection, anomaly detection, or system monitoring\u2014basically, those decisions made by machines with quick turnaround or humans staring at computer screens as their day job.\nThat leaves us with a wide chasm of five-minute to one-hour end-to-end processing latency, which I refer to in this post as near-real-time. Most such cases are either powering business dashboards and/or aiding some human decision-making. Here are some examples where near-real-time could be applicable:\u00a0\n\nObserving whether something was anomalous across the board in the last x minutes;\nGauging how well the experiments running on the website performed in the last x minutes;\nRolling up business metrics at x-minute intervals\nExtracting features for a machine-learning pipeline in the last x minutes.\n\n\nFigure 1. Different shades of processing latency with the typical technologies used therein. Source: Vinoth Chandar.\n\nIncremental processing via \u201cmini\u201d batches\nThe choices to tackle near-real-time use cases are pretty open ended. Stream processing can provide low latency, with budding SQL capabilities, but it requires the queries to be predefined to work well. Proprietary warehouses have a lot of features (e.g., transactions, indexes) and can support ad hoc and predefined queries, but such proprietary warehouses are typically limited in scale and are expensive. Batch processing can tackle massive scale and provides mature SQL support via Spark SQL/Hive, but the processing styles typically involve larger latency. With such fragmentation, users often end up making their choices based on available hardware and operational support within their organizations. We will circle back to these challenges at the conclusion of this post.\nFor now, I\u2019d like to outline some technical benefits to tackling near-real-time use cases via \u201cmini\u201d batch jobs run every x minutes, using Spark/MR as opposed to running stream-processing jobs. Analogous to \u201cmicro\u201d batches in Spark Streaming (operating at second-by-second granularity), \u201cmini\u201d batches operate at minute-by-minute granularity. Throughout the post, I use the term \u201cincremental processing\u201d collectively to refer to this style of processing.\nIncreased efficiency\nIncrementally processing new data in \u201cmini\u201d batches could be a much more efficient use of resources for the organization. Let\u2019s take a concrete example, where we have a stream of Kafka events coming in at 10K/sec and we want to count the number of messages in the last 15 minutes across some dimensions. Most stream-processing pipelines use an external result store (e.g., Cassandra, ElasticSearch) to keep aggregating the count, and keep the YARN/Mesos containers running the whole time. This makes sense in the less-than-five-minute latency windows such pipelines operate on. In practice, typical YARN container start-up costs tend to be around a minute. In addition, to scale the writes to the result stores, we often end up buffering and batching updates, and this protocol needs the containers to be long running.\n\nFigure 2. Comparison of processing via stream processing engines vs incremental \u201cmini batch\u201d jobs. Source: Vinoth Chandar.\n\nHowever, in the near-real-time context, these decisions may not be the best ones. To achieve the same effect, you can use short-lived containers and improve the overall resource utilization. In the figure above, the stream processor performs 6M updates over 15 minutes to the result store. But in the incremental processing model, we perform in-memory merge once and only one update to the result store, while using the containers for only five minutes. The incremental processing model is three times more CPU efficient, several magnitudes more efficient on updating of the result store. Basically, instead of waiting for work and eating up CPU and memory, the processing wakes up often enough to finish up pending work, grabbing resources on demand.\nBuilt on top of existing SQL engines\nOver time, a slew of SQL engines have evolved in the Hadoop/big data space (e.g., Hive, Presto, SparkSQL) that provide better expressibility for complex questions against large volumes of data. These systems have been deployed at massive scale and hardened over time in terms of query planning, execution, and so forth. On the other hand, SQL on stream processing is still in early stages. By performing incremental processing using existing, much more mature SQL engines in the Hadoop ecosystem, we can leverage the solid foundations that have gone into building them.\nFor example, joins are very tricky in stream processing, in terms of aligning the streams across windows. In the incremental processing model, the problem naturally becomes simpler due to relatively longer windows, allowing more room for the streams to align across a processing window. On the other hand, if correctness is more important, SQL provides an easier way to expand the join window selectively and reprocess.\nAnother important advancement in such SQL engines is the support for columnar file formats like ORC/Parquet, which have significant advantages for analytical workloads. For example, joining two Kafka topics with Avro records would be much more expensive than joining two Hive/Spark tables backed by ORC/Parquet file formats. This is because with Avro records, you would end up de-serializing the entire record, whereas columnar file formats only read the columns in the record that are needed by the query. For example, if we are simply projecting out 10 fields out of a total 1,000 in a Kafka Avro encoded event, we still end up paying the CPU and IO cost for all fields. Columnar file formats can typically be smart about pushing the projection down to the storage layer.\n\nFigure 3. Comparison of CPU/IO cost of projecting 10 fields out of 1000 total, as Kafka events vs. columnar files on HDFS. Source: Vinoth Chandar.\n\nFewer moving parts\nThe famed Lambda architecture that is broadly implemented today has two components: speed and batch layers, usually managed by two separate implementations (from code to infrastructure). For example, Storm is a popular choice for the speed layer, and MapReduce could serve as the batch layer. In practice, people often rely on the speed layer to provide fresher (and potentially inaccurate) results, whereas the batch layer corrects the results of the speed layer at a later time, once the data is deemed complete. With incremental processing, we have an opportunity to implement the Lambda architecture in a unified way at the code level as well as the infrastructure level.\n\nFigure 4. Computation of a result table, backed by a fast view via incremental processing and a more complete view via traditional batch processing. Source: Vinoth Chandar.\n\nThe idea illustrated in the figure above is fairly simple. You can use SQL, as discussed, or the same batch processing framework as Spark to implement your processing logic uniformly. The resulting table gets incrementally built by way of executing the SQL on the \u201cnew data\u201d just like stream processing, to produce a \u201cfast\u201d view of the results. The same SQL can be run periodically on all of the data to correct any inaccuracies (remember, joins are tricky!), to produce a more \u201ccomplete\u201d view of the results. In both cases, we will be using the same Hadoop infrastructure for executing computations, which can bring down overall operational cost and complexity.\nChallenges of incremental processing\nHaving laid out the advantages of an architecture for incremental processing, let\u2019s explore the challenges we face today in implementing this in the Hadoop ecosystem.\nTrade-off: Completeness vs. latency\nIn computing, as we traverse the line between stream processing, incremental processing, and batch processing, we are faced with the same fundamental trade-off. Some applications need all the data and produce more complete/accurate results, whereas some just need data at lower latency to produce acceptable results. Let\u2019s look at a few examples.\n\nFigure 5. Figure showing different Hadoop applications on their tolerance for latency and completeness. Source: Vinoth Chandar.\n\nFigure 5 depicts a few sample applications, placing them according to their tolerance for latency and (in)completeness. Business dashboards can display metrics at different granularities because they often have the flexibility to show more incomplete data at lower latencies for recent times while over time getting complete (which also made them the marquee use-case for Lambda architecture). For data science/machine learning use cases, the process of extracting the features from the incoming data typically happens at lower latencies, and the model training itself happens at a higher latency with more complete data. Detecting fraud, on one hand, requires low-latency processing on the data available thus far. An experimentation platform, on the other hand, needs a fair amount of data, at relatively lower latencies, to keep results of experiments up to date.\nThe most common cause for lack of completeness is late-arriving data (as explained in detail in this Google Cloud Dataflow deck). In the wild, late data can manifest in infrastructure-level issues, such as data center connectivity flaking out for 15 minutes, or user-level issues, such as a mobile app sending late events due to spotty connectivity during a flight. At Uber, we face very similar challenges, as we presented at Strata + Hadoop World earlier this year.\nTo effectively support such a diverse set of applications, the programming model needs to treat late-arrival data as a first-class citizen. However, Hadoop processing has typically been batch-oriented on \u201ccomplete\u201d data (e.g., partitions in Hive), with the responsibility of ensuring completeness also resting solely with the data producer. This is simply too much responsibility for individual producers to take on in today\u2019s complex data ecosystems. Most producers end up using stream processing on a storage system like Kafka to achieve lower latencies, while relying on Hadoop storage for more \u201ccomplete\u201d (re)processing. We will expand on this in the next section.\nLack of primitives for incremental processing\nAs detailed in this article on stream processing, the notions of event time versus arrival time and handling of late data are important aspects of computing with lower latencies. Late data forces recomputation of the time windows (typically, Hive partitions in Hadoop), over which results might have been computed already and even communicated to the end user. Typically, such recomputations in the stream processing world happen incrementally at the record/event level by use of scalable key-value stores, which are optimized for point lookups and updates. However, in Hadoop, recomputing typically just means rewriting the entire (immutable) Hive partition (or a folder inside HDFS for simplicity) and recomputing all jobs that consumed that Hive partition.\nBoth of these operations are expensive in terms of latency as well as resource utilization. This cost typically cascades across the entire data flow inside Hadoop, ultimately adding hours of latency at the end. Thus, incremental processing needs to make these two operations much faster so that we can efficiently incorporate changes into existing Hive partitions as well as provide a way for the downstream consumer of the table to obtain only the new changes.\nEffectively supporting incremental processing boils down to the following primitives:\nUpserts: Conceptually, rewriting the entire partition can be viewed as a highly inefficient upsert operation, which ends up writing way more than the amount of incoming changes. Thus, first-class support for (batch) upserts becomes an important tool to possess. In fact, recent trends like Kudu and Hive Transactions do point in this direction. The Google Mesa paper also talks about several techniques that can be applied in the context of ingesting quickly.\nIncremental consumption: Although upserts can solve the problem of publishing new data to a partition quickly, downstream consumers do not know what data has changed since a point in the past. Typically, consumers learn this by scanning the entire partition/table and recomputing everything, which can take a lot of time and resources. Thus, we also need a mechanism to more efficiently obtain the records that have changed since the last time the partition was consumed.\nWith the two primitives above, you can support a lot of common-use cases by upserting one data set and then incrementally consuming from it to build another data set incrementally. Projections are the most simple to understand, as depicted in Figure 6.\n\nFigure 6. Simple example of building of table_1 by upserting new changes, and building a simple projected_table via incremental consumption. Source: Vinoth Chandar.\n\nBorrowing terminology from Spark Streaming, we can perform simple projections and stream-data set joins much more efficiently at lower latency. Even stream-stream joins can be computed incrementally, with some extra logic to align windows.\n\nFigure 7. More complex example that joins a fact table against multiple dimension tables, to produce a joined table. Source: Vinoth Chandar.\n\nThis is actually one of the rare scenarios where we could save money with hardware while also cutting down the latencies dramatically.\nShift in mindset\nThe final challenge is not strictly technical. Organizational dynamics play a central role in which technologies are chosen for different use cases. In many organizations, teams pick templated solutions that are prevalent in the industry, and teams get used to operating these systems in a certain way. For example, typical warehousing latency needs are on the order of hours. Thus, even though the underlying technology could solve a good chunk of use cases at lower latencies, a lot of effort needs to be put into minimizing downtimes or avoiding service disruptions during maintenance. If you are building toward lower latency SLAs, these operational characteristics are essential. On the one hand, teams that solve low-latency problems are extremely good at operating those systems with strict SLAs, and invariably the organization ends up creating silos for batch and stream processing, which impedes realization of the aforementioned benefits to incremental processing on a system like Hadoop.\nThis is in no way an attempt to generalize the challenges of organizational dynamics, but is merely my own observation as someone who has spanned the online services powering LinkedIn as well as the data ecosystem powering Uber.\nTakeaways\nI would like to leave you with the following takeaways:\n\nGetting really specific about your actual latency needs can save you tons of money.\nHadoop can solve a lot more problems by employing primitives to support incremental processing.\nUnified architectures (code + infrastructure) are the way of the future.\n\nAt Uber, we have very direct and measurable business goals/incentives in solving these problems, and we are working on a system that addresses these requirements. Please feel free to reach out if you are interested in collaborating on the project.\nContinue reading Uber\u2019s case for incremental processing on Hadoop.",
  "title": "Uber\u2019s case for incremental processing on Hadoop",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vPkUb-5tyic/ubers-case-for-incremental-processing-on-hadoop"
 },
 {
  "content": "Make Music, HTTP/2 Push, d3 Scaffolding, and Elan Lee Interview\n\nMarble Musical Machine (YouTube) -- beautiful example of the nerd tinkerer's art.\n\nRules of Thumb for HTTP/2 Push -- The intended audience is HTTP/2 server implementers, who want to support server push in their servers, and web developers, who think their pages may be benefit from server push. (via Eric Bidelman)\n\nHow I Start d3 Projects (Chris McDowall) -- This is the little bit of scaffolding I typically use when starting a d3.js project.\n\n\nInterview with Elan Lee -- podcast, no transcription, unfortunately. It used to be that you would exchange money for a product. Now, it's becoming much more attractive to exchange money for an experience.\n\n\nContinue reading Four short links: 4 August 2016.",
  "title": "Four short links: 4 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/zLp3SJ0Sh40/four-short-links-4-august-2016"
 },
 {
  "content": "Community labs are the forefront of the DIYbio movement.\nBergen McMurray has cherry-red hair and shoulders painted with black ink tattoos. You probably wouldn\u2019t be surprised to learn that she\u2019s worked as an artist, a photographer, and a graphic designer. But McMurray has a few more titles that set her apart. She\u2019s a loving mother, she\u2019s published in Nature, and in her spare time, she happens to lead a buzzing colony of do-it-yourself biologists.\nDo-it-yourself biology, or DIYbio, is part of a growing movement which seeks to give people the tools to learn without formal education. \u201cPunk,\u201d \u201cmaker,\u201d or \u201chacker\u201d\u2014the DIY community can go by many names, all of which embody the same directive: democratize science. DIYbio empowers the individual to seek scientific knowledge through trial and error, getting hands dirty in books and bench work alike. In the same way computer tinkering and garage electronics powered the human element of the digital revolution, DIYbio provides the average individual a pathway to biological literacy.Continue reading HiveBio: Access Granted.",
  "title": "HiveBio: Access Granted",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Tm4MIdEBtuo/hivebio-access-granted"
 },
 {
  "content": "The O\u2019Reilly Security Podcast: Vulnerabilities in assembled software and the need for immediate developer feedback.In this episode, I talk with Chris Eng, vice president of research at Veracode, a software security-as-a-service business.\nWe discuss Veracode\u2019s research on application security across a broad spectrum of industries, the challenges of securing modern \u201cassembled\u201d software, and making it easier for developers to bake in security from the get-go.Continue reading Chris Eng on the challenges of improved application security.",
  "title": "Chris Eng on the challenges of improved application security",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/MlfeQRoO2Go/chris-eng-on-the-challenges-of-improved-application-security"
 },
 {
  "content": "Aussie Startups, Exponential Tech, Server-side TLS, and Hololens GA\n\nAustralian Startup Scene (Tristan Pollock) -- the Australia and New Zealand VC ecosystem has grown eight times bigger since 2011 and broke $600 million in funding for the first time ever.\n\n\nCartoon Intro to Exponential Tech (Kaila Colbin) -- the human brain's struggle with exponential series is the governor on our imagination. This cartoon does a nice job of releasing the brakes for those who haven't yet had their thinking opened up.\n\nServer-Side TLS (Mozilla) -- The Operations Security (OpSec) team maintains this document as a reference guide to navigate the TLS landscape. It contains information on TLS protocols, known issues and vulnerabilities, configuration examples, and testing tools.\n\n\nHololens Goes GA -- USD3k per, max 5/customer. USA and Canada only.\n\nContinue reading Four short links: 3 August 2016.",
  "title": "Four short links: 3 August 2016",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/TH27CnmDABQ/four-short-links-3-august-2016"
 },
 {
  "content": "Objectives and Key Results (OKRs) have helped organizations like Google and LinkedIn achieve their goals.  Learn what OKRs are and how to apply them to your business.\nIntroduction\nWhy is there so much interest in Objectives and Key Results, or OKRs? After all, OKRs are just a goal-setting methodology. When Silicon Valley startups discovered OKRs were behind the meteoric rise of companies such as Google, LinkedIn, Twitter, and Zynga, company after company decided to adopt OKRs, hoping to catch even a fraction of that success. But they struggled. The knowledge of how to use OKRs effectively was lore, passed on from employees who often had a partial understanding of how and why they worked. Many companies failed to use them successfully and then abandoned them with the same alacrity with which they adopted them.\nThere is no question that OKRs work. The mystery is why they don\u2019t work for everyone. This report will share how the best companies use them to create focus, unity, and velocity.Continue reading Introduction to OKRs.",
  "title": "Introduction to OKRs",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/kDcLaK2y6iY/introduction-to-okrs"
 }
]